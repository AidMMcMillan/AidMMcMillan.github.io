[
  {
    "objectID": "posts/final_project_blog/report.html",
    "href": "posts/final_project_blog/report.html",
    "title": "Final Project: Classifying MiddCourses Reviews",
    "section": "",
    "text": "We developed review quality classifiers based on text and metadata features from course reviews on MiddCourses, a student course evaluation site for Middlebury College. Using human annotation, split between 3 annotators, we determined the quality levels of reviews from a dataset of roughly 1,900 reviews. We fixed a annotation threshold such that our resulting binary labels (high and low review quality) were roughly equal. We then built and tested over 50 features and selected the top features based on their correlation with our quality labels and on the level of difference in the kernel density distribution of the feature when faceted by our quality label. We trained a logistic regression classifier on these engineered features and achieved an f1 score of about 0.75 for each class. We also trained XLNet, a transformer based, model on the text content of our reviews, achieving an f1 score of 0.65 for low-quality reviews and 0.72 for high-quality reviews.\nOur source code is available at https://github.com/Nicholas-Sliter/csci0451-final-project."
  },
  {
    "objectID": "posts/final_project_blog/report.html#related-work",
    "href": "posts/final_project_blog/report.html#related-work",
    "title": "Final Project: Classifying MiddCourses Reviews",
    "section": "Related Work",
    "text": "Related Work\nWe started our research by looking at existing work on review helpfulness prediction. We found that there are two main approaches to this problem:\n\nMetadata-based: uses metadata and computed features to predict helpfulness (Du et al. 2019; Singh et al. 2017; Mauro, Ardissono, and Petrone 2021; Liu et al. 2007)\nText-based: uses NLP-based methods or deep learning on review text to predict helpfulness (Mohawesh et al. 2021; Salminen et al. 2022; Lubis, Rosmansyah, and Supangkat 2017)"
  },
  {
    "objectID": "posts/final_project_blog/report.html#data",
    "href": "posts/final_project_blog/report.html#data",
    "title": "Final Project: Classifying MiddCourses Reviews",
    "section": "Data",
    "text": "Data\nWe used fully anonymous (user-unaware) data from MiddCourses of reviews submitted prior to 2023-04-21 18:09:01.856+00. This data was provided to us by the MiddCourses team. The SQL query used to gather the data is available on GitHub in the code repository.\nEach row represents a review of a course-instructor pair.\nA full datasheet is available at https://github.com/Nicholas-Sliter/middcourses2#review-structure.\n\nLimitations\nThe review data is UGC (user-generated content) and therefore has the following limitations:\n\nResponse Bias: The data is biased towards users who are more likely to leave reviews. This means that the data is not representative of the entire population of Middlebury students. This is a common problem with UGC and is difficult to mitigate.\nExtremity Bias: As review data, MiddCourses reviews exhibit extremity bias. This means that reviews are more likely to be either very positive or very negative. This is a well-known problem with review data and is difficult to mitigate.\nAcquiescence Bias: Reviewers may feel pressured to leave a positive review for a course or instructor. This is especially true for courses and instructors that are well-liked.\n\nReviewers may also feel compelled to leave reviews due to the incentive structure of the site. These reviews may not exhibit an accurate representation of the course or instructor. Indeed, it is these reviews that we want to identify and de-rank."
  },
  {
    "objectID": "posts/final_project_blog/report.html#approach",
    "href": "posts/final_project_blog/report.html#approach",
    "title": "Final Project: Classifying MiddCourses Reviews",
    "section": "Approach",
    "text": "Approach\nWe approached the problem by conducting experiments with feature engineering and NLP methods to identify a set of features that would effectively classify reviews based on quality. On top of that, we trained our data on a deep language model (XLNet) to compare those results to the results of our own feature engineering. Our objective was to design a classification model that most accurately predicted review quality based on the input data.\n\n\n\nTo begin, we performed triple-annotation on the entire dataset using an annotation tool (midd.courses/annotate) and transformed the annotations into classification labels based on a certain threshold. Since there were no inherent labels for review quality, we relied on annotations as the ground truth. Recognizing the subjectivity of this process, we each individually annotated the data to mitigate bias.\n\nExample of a review on the annotation tool\n\nIn order to ensure that our annotations were consistent, we created an annotation rubric. The rubric gave us a rough idea of whether to give the annotation a thumbs up (+1), thumbs down (-1), or leave it blank (0):\n\n\n\n\n\n\n\n\nGreat review (+1)\nOk review (+0)\nBad review (-1)\n\n\n\n\nText consistent with sliders\nText mostly consistent with sliders\nText is not consistent with sliders\n\n\nText is well written\nText is well written\nText is difficult to read\n\n\nGives a balanced picture of the course\nGives a limited perspective on the course (e.g. only negative)\nGives a very limited perspective on the course (e.g. hyperbolically negative)\n\n\nText offers valuable information not found in the sliders and other questions\nText does not offer much information about the course not found in the sliders and other questions\nText is irrelevant to the course or contains irrelevant meta information (e.g. personal information, spam, padding to make character count, etc)\n\n\n\nThis rubric gave us a consistent guideline by which to score the reviews. Once the annotations were complete, each review received a vote score which was the sum of the annotation values. We then decided on the threshold of 0 to create our target vector \\(y\\). Any review below this threshold received an imputed quality of 0 (indicating lower quality) and any review above the threshold received an imputed quality of 1.\n\nMetadata-based approach (Feature Engineering & Logistic Regression):\nFor the initial model, we engineered features using both the textual content of the reviews and the non-textual metadata associated with each review. This process entailed brainstorming and developing the features, evaluating their correlation with the imputed_quality (our target vector), and assessing the intercorrelation among the features to mitigate multicollinearity. We then chose the highest performing features and ran them through a linear regression model.\nThe list below shows all of the features we selected for our classification task.\nMetadata-based:\n\n\n\n\n\n\n\n\nFeature\nDescription\nQual./Quant.\n\n\n\n\ndefault_mse\nMean squared error between default slider values and the slider values of the review\nquant.\n\n\noverly_negative_mse\nMean squared error between overly negative slider values and the slider values of the review\nquant.\n\n\nrating_diff\nDifference between the default value of the rating slider and the value of the rating slider for the review\nquant.\n\n\nwasAuthorized\nUser had authorization to view other reviews at the time they submitted their review\nqual.\n\n\n\nText-based:\n\n\n\n\n\n\n\n\nFeature\nDescription\nQual./Quant.\n\n\n\n\ndefault_mse\nMean squared error between default slider values and the slider values of the review\nquant.\n\n\noverly_negative_mse\nMean squared error between overly negative slider values and the slider values of the review\nquant.\n\n\nrating_diff\nDifference between the default value of the rating slider and the value of the rating slider for the review\nquant.\n\n\ncontent_larger_than_250\nText contains more than 250 characters\nqual.\n\n\ncontent_length\nNumber of characters in text (normalized)\nquant.\n\n\nnumber_count\nNumber of numbers in text\nquant.\n\n\nmentions_prof_word\nText mentions any word related to “professor”\nqual.\n\n\ncontains_pronouns\nText contains pronouns\nqual.\n\n\ncontains_personal_pronouns\nText contains personal pronouns\nqual.\n\n\naverage_sentence_length\nAverage length of sentence in text\nquan.\n\n\nfre_readability\nFlesch Reading Ease, a formula that measures the text readability (from 0-100) based on the average length of your sentences (measured by the number of words) and the average number of syllables per word\nquant.\n\n\nsmog_readability\nSMOG index, a readability formula that assesses the “grade-level” of text based on the number of polysyllabic words\nquant.\n\n\ndale_chall_readability\nDale-Chall readability formula, a method used to determine the approximate “grade-level” of a text based on sentence length and the number of “hard” words\nquant.\n\n\nentropy\nShannon entropy based on the frequency of each character in the text\nquant.\n\n\nword_entropy\nShannon entropy based on the frequency of each word in the text\nquant.\n\n\nsentiment_scores\nSentiment scores calculated using vaderSentiment’s SentimentIntensityAnalyzer\nquant.\n\n\nneutrality_scores\nNeutrality scores calculated using vaderSentiment’s SentimentIntensityAnalyzer\nquant.\n\n\n\nFinally, we also included as a feature a vectorized version of the text. The vectorizer applies TF-IDF (Term Frequency-Inverse Document Frequency) weighting to prioritize important words while filtering out common and less informative ones, using specified parameters such as maximum features, maximum document frequency, minimum document frequency, and a set of English stop words.\nIn order to select the best features, we looked at the coorelation between features and imputed_quality and feature intercoorelation. Furthermore, we looked at density plots to discover quantitative features that caused different distributions for reviews with differing imputed_quality.\n\nDensity plots for two features of varying quality\n\n\nCorrelation matrix of our selected features\n\n\n\nText-based approach (XLNet):\nIn an attempt to achieve better results, we also experimented with using a deep model to classify reviews. While exploring the possible options for deep NLP, we came across multiple sequence-in architectures such as Recurrent Neural Networks (RNNs), Long-Short Term Memory (LSTM) and Gated Recurrent Units (GRU). Eventually we decided to implement the Transformer architecture, one of the most prominent recent advancements in deep learning.\nIn a 2017 paper titled “Attention is All You Need,” the the Transformer architecture was proposed by a team led by Ashish Vaswami (Vaswani et al. 2017). It has since become the go-to method for a wide range of NLP tasks and it’s probably most well known for its use in GPT-3.\nUnlike previous NLP approaches that often relied on encoder-decoder recurrent neural networks (RNNs) for sequence modeling, the Transformer architecture introduced an attention mechanism between the encoder and decoder (Cristina 2023b). Attention is a method that allows the neural network to focus on only the most important parts of the input sequence. Furthermore, it eliminates the issue of diminishing gradients found when using RNNs. In other words, accuracy does not drop when the number of inputs in the sequence increases.\n\nTypical sequence-in sequence-out RNN with encoder and decoder from “Sequence to Sequence Learning with Neural Networks”\n\n\nEncoder decoder with attention from “Attention in Psychology, Neuroscience, and Machine Learning”\n\nBy using self-attention mechanisms, the Transformer architecture avoids the sequential nature of RNNs, making it highly parallelizable and quickly optimizable (through backpropagation). It therefore trains very efficiently on parallel processors like GPUs (Cristina 2023b).\n\nFull transformer model from “Attention is All You Need”\n\nOur specific model utilizes XLNet. XLNet is an open-source Transformer architecture that has been pre trained using permutation-based autoregressive training to address limitations of the traditional autoregressive models, mainly the lack of bidirectional context (Yang et al. 2020; Cristina 2023a). This modification enables XLNet to better capture bidirectional context, leading to improved performance on several NLP tasks.\nWe accessed XLNet through SimpleTransformers based on the Transformers library by HuggingFace. This allowed us to quickly implement the Transformer architecture and then tune the parameters to figure out the best outcomes. We created the model to take in text as a sequence of words (which are embedded) and then output either 0 or 1 as the predicted imputed_quality. We also trained only the final layer of the model.\nSimple transformers documentation: https://simpletransformers.ai/docs/installation/\nFurther reading on Transformer architecture: - https://machinelearningmastery.com/a-tour-of-attention-based-architectures/ - https://machinelearningmastery.com/the-transformer-model/"
  },
  {
    "objectID": "posts/final_project_blog/report.html#nicholas",
    "href": "posts/final_project_blog/report.html#nicholas",
    "title": "Final Project: Classifying MiddCourses Reviews",
    "section": "Nicholas",
    "text": "Nicholas\nI worked on much of the feature engineering for the metadata-based model. I also worked on the data collection and annotation, building the annotation tool on MiddCourses and providing the data. For the final report, I worked on the abstract, introduction, values-statement, and part of the methods sections."
  },
  {
    "objectID": "posts/final_project_blog/report.html#paul",
    "href": "posts/final_project_blog/report.html#paul",
    "title": "Final Project: Classifying MiddCourses Reviews",
    "section": "Paul",
    "text": "Paul\nI worked on annotating the reviews on MiddCourses website and finding ways to measure lexical richness using a Python module called LexicalRichness. For the final report I worked on the results in addition to parts of the conclusion."
  },
  {
    "objectID": "posts/final_project_blog/report.html#aidan",
    "href": "posts/final_project_blog/report.html#aidan",
    "title": "Final Project: Classifying MiddCourses Reviews",
    "section": "Aidan",
    "text": "Aidan\nI worked on researching deep NLP methods and implementing and fine-tuning the XLnet. I also annotated the reviews and helped out a little with the feature engineering. For the final report I wrote both the metadata and text-based part of the approach section."
  },
  {
    "objectID": "posts/blog1/blog_1.html",
    "href": "posts/blog1/blog_1.html",
    "title": "Blog 1: Perceptron Algorithm",
    "section": "",
    "text": "Implementing the Perceptron Algorithm\nIn order to implement the perceptron algorithm in python, I created a perceptron class. Within that class I defined the fit method:\nfit(self, X, y, max_steps)\nBefore iterating through the algorithm, I first made an array \\(\\tilde{\\mathbf{X}} = [\\mathbf{X}, \\mathbf{1}]\\) and initialed the vector \\(\\tilde{\\mathbf{w}}\\) with random values from \\(0-1\\):\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\nself.w = np.random.rand(X_.shape[1])\nAfter initializing \\(\\tilde{\\mathbf{X}}\\) and \\(\\tilde{\\mathbf{w}}\\), I iterated between random vectors in \\(\\tilde{\\mathbf{X}}\\) and updated \\(\\tilde{\\mathbf{w}}\\) using the equation:\n\\[\\tilde{\\mathbf{w}}^{(t+1)} = \\tilde{\\mathbf{w}}^{(t)} + \\mathbb{1}(\\tilde{y}_i \\langle \\tilde{\\mathbf{w}}^{(t)}, \\tilde{\\mathbf{x}}_i\\rangle < 0)\\tilde{y}_i \\tilde{\\mathbf{x}}_i\\]\nIn python that update looks looks like:\nself.w = self.w + (1*((y_[i]*(self.w@X_[i]))<0))*(y_[i]*X_[i])\n\n\nExperiment 1: Linearly Seperable Data\nUsing the make_blobs() fuction, I created two linearly seperable groups of data. I then created an instance of the perceptron class and called the fit method on the data. Plotting both the data and the hyperplane (line) that seperated the data makes it clear that this test resultued in a success:\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom perceptron import Perceptron\n\nfrom sklearn.datasets import make_blobs, make_circles\n\nnp.random.seed(12345)\n\nX, y = make_blobs(n_samples = 100, n_features = 2, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nLooking at the last few accuracy scores we can also see that the perceptron algorithm converged and reached 100% accuracy.\n\nprint(p.history[-10:])\n\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n\n\nThis graph shows the full progression of the acuracy throughout all iterations of the algorithm. We can clealy see that the algorithm converged and it finished before reaching max_steps=1000.\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\nExperiment 2 & 3: Perceptron Algorithm on Non-linearly Seperable Data\nIn experiment two, I used the make_blobs() function but put the centers of the blobs closer together so that they would have overlapping data. As seen in the figure below, the two sets of data are not linearly seperable. We can see that the line seperates the data to some extent but does not completely seperate the data because that would be impossible. This is the line achieved after 1000 iterations of the algorithm.\n\nX, y = make_blobs(n_samples = 100, n_features = 2, centers = [(-1, -1), (1, 1)])\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nBelow we also see that perfect accuracy is not acheived after 1000 iterations. Furthermore, the accuracy also does not consistantly improve with each iteration. It even drops from 97% to 94% in the last two iterations.\n\nprint(p.history[-10:])\n\n[0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.94, 0.94]\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nIn experiment three I created another non linear data set using the make_circles() function. This results in a two sets of data in the shape of a circle, one encompassing the other. From this experiment we also see that it is impossible to seperate the two sets of data with a hyperplane.\nI chose to include this experiment as well in order to highlight the downsides to using perceptrons to seperate data.\nWhile there are cases in which non-linearly seperable data sets are still roughly seperable by a hyperplane (such as experiment 2) there are cases like the one below where a hyperplane would not even be helpful in predicting the labels of the data.\n\nX, y = make_circles(200, shuffle = True, noise = 0.1, factor = 0.5)\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nBelow we see that the acuracy hovers around 50% and does not improve with each iteration. Because we used binary classifiers, a 50% accuracy rate means that this peceptron is just as good as guessing and is therefore not even helpful in classifying the data sets.\n\nprint(p.history[-10:])\n\n[0.475, 0.475, 0.475, 0.5, 0.455, 0.455, 0.455, 0.455, 0.5, 0.49]\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\nEperiment 4: Perceptron Algorithm on 5-dimensional Data\nIn my final experiment, I ran the perceptron algorithm on data with 5 features instead of two. Unfortionally, such data is hard to represent visually so because of the dimensionality. I created the set using make_blobs() with 5 features instead of 2 and centers at (-1, -1, -1, -1, -1) and (1.7, 1.7, 1.7, 1.7, 1.7).\n\nnp.random.seed(123)\n\nX, y = make_blobs(n_samples = 100, n_features = 5, \n                  centers = [(-1, -1, -1, -1, -1), \n                             (1.7, 1.7, 1.7, 1.7, 1.7)])\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\nBelow we see the weights and biases of \\(\\tilde{\\mathbf{w}}\\).\n\nprint(p.w)\n\n[ 3.26420494  0.99757888  1.62033846  1.70676209  0.21630719 -3.72850816]\n\n\nAs shown below, this experiment proved sucessful since the algorithm converged finding a hyperplane that seperated the data with perfect accuracy. This also means our data generated using the make_blobs() function was linearly seperable.\n\nprint(p.history[-10:])\n\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\nRuntime Complexity of Single Iteration of the Perceptron Algorithm\nIn order to calculate the runtime complexity of the one iteration of the perceptron algorithm I must look at the equation used to update \\(\\tilde{\\mathbf{w}}\\):\n\\[\\tilde{\\mathbf{w}}^{(t+1)} = \\tilde{\\mathbf{w}}^{(t)} + \\mathbb{1}(\\tilde{y}_i \\langle \\tilde{\\mathbf{w}}^{(t)}, \\tilde{\\mathbf{x}}_i\\rangle < 0)\\tilde{y}_i \\tilde{\\mathbf{x}}_i\\]\nFirstly, \\(\\tilde{\\mathbf{w}}^{(t)}\\) and \\(\\tilde{\\mathbf{x}}_i\\) are both \\(p+1\\) dimensional vectors, where \\(p\\) is the number of features.\nBy definition, the dot product of two \\(n\\)-dimensionsal vectors \\(a\\) and \\(b\\) is: \\[a_1*b_1 + a_2*b_2 \\space ... + \\space a_n*b_n\\]\nWithin this calculation, \\(n\\) multiplications are performed and \\(n-1\\) additions are performed. Therefore, when \\(\\langle \\tilde{\\mathbf{w}}^{(t)}, \\tilde{\\mathbf{x}}_i\\rangle\\) is calculated, \\(p+1\\) multiplications and \\(p\\) additions are performed.\nTherefore, the complexity of performing the dot product of \\(\\tilde{\\mathbf{w}}^{(t)}\\) and \\(\\tilde{\\mathbf{x}}_i\\) is \\(2p+1=O(p)\\).\nFurthermore, multiplying that dot product by \\(\\tilde{y}_i\\), the \\(<\\) comparison, and the \\(\\mathbb{1}()\\) function are all \\(O(1)\\).\nLastly, the scalar multiplication on \\(\\tilde{\\mathbf{x}}_i\\) requires \\(p+1\\) multilplications and is therefore \\(O(p)\\).\nThat means that the final runtime complexity of a single iteration of the perceptron algorithm is \\(2*O(p) + 3*O(1) = O(p)\\).\nThe runtime complexity of this single operation is therefore not dependent on the number of data points in the set but instead only on the number of features."
  },
  {
    "objectID": "posts/blog8/blog_8.html",
    "href": "posts/blog8/blog_8.html",
    "title": "Blog 8: Learning From Dr. Timnit Gebru",
    "section": "",
    "text": "On Monday, 4/21 Dr. Timnit Gebru will be joining our class via Zoom to have a discussion with us about her research and areas of expertise. Later that night, she will be doing a talk open to all where she will discuss the topic of “Eugenics and the Promise of Utopia through Artificial General Intelligence.\n\n\n\nDr. Timnit Gebru is a computer scientist and AI researcher. A lot of her work focuses on ethical considerations of AI, especially as it relates to machine bias. She is a leading advocate for BIPOC in computer science. After taking note of the extreme lack of diversity in the field while attending an annual AI conference, she co-founded Black in AI, a community of Black researchers in the field of AI.\nGebru has published research on a variety of topics related to AI, including bias in facial recognition technology and the lack of diversity in AI datasets. She is also recognized for her paper titled “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?” where she writes about the possible risks of large language models like GPT-3 such as its environmental impacts, concentration of power, amplification of bias and threats to privacy.\nAllegedly, the contents of that article and her expression of concerns over Google’s workforce diversity lead to her being fired from her position at Google as a research scientist and co-leader of the Ethical AI team. Since then, she has launched her own institute called the Distributed Artificial Intelligence Research Institute (DAIR) in order to further her research on the effects of AI on marginalized communities, specifically people of African descent.\n\n\n\nIn her talk “Computer vision in practice: who is benefiting and who is being harmed?” Dr. Gebru discusses the issues of machine bias in the computer vision field. She specifically discussed the problematic nature of using facial analysis softwares in hiring decisions and criminal justice. She then discussed the issues with the lack of diversity in data sets and how that can cause problems. On top of that, many organizations use dubious methods to scrape data on darker skinned faces and often in the process use that data in algorithms that do not benefit those people. Finally, she discusses the lack of diversity in the people who are behind creating these algorithms.\ntl:dr Computer vision has many ethical drawbacks especially when it comes to racial disparities in facial recognition software, something that can further marginalize already marginalized communities.\n\n\n\nIn 2020, your firing from Google sparked a widespread debate about the need for greater transparency and accountability in the tech industry. What changes do you think must be made in the industry to ensure that AI technology, and specifically large speech models such as GPT-3, are developed and deployed in both a responsible and ethical way?"
  },
  {
    "objectID": "posts/blog2/blog_2.html",
    "href": "posts/blog2/blog_2.html",
    "title": "Blog 2: Optimization of Logistic Regression",
    "section": "",
    "text": "Finding Gradient of Emperical Risk Funtion\nIn order to calculate the gradient descent of the emprical risk fuction, I first had to define the derivitive of the logistic loss function:\n    def d_logistic_loss(self, y_hat, y):\n        return self.sigmoid(y_hat) - y\nThen I defined the gradient as:\n    def gradient(self, X, y, d_loss):\n        return np.mean(np.swapaxes(X, 0, 1)*d_loss(X@self.w, y), axis = 1) \nI used np.swapaxes() to flip the first and second axes of \\(\\tilde{\\mathbf{X}}\\). That allowed me to be able to multiply each \\(\\mathbf{x}_i\\) in \\(\\tilde{\\mathbf{X}}\\) by \\(\\frac{d\\ell(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle, y_i)}{d\\hat{y}}\\). I then found the mean of that entire vector over the second axis (because the axes were flipped) which left me with the gradient vector of the same legth as \\(\\mathbf{w}\\).\nThis gives us the gradient descent of the emprical risk fuction on \\(\\mathbf{w}\\) because of the derivation we saw in class:\n\\[\\begin{align}\n\\nabla L(\\mathbf{w}) &= \\nabla \\left(\\frac{1}{n} \\sum_{i = 1}^n \\ell(f_{\\mathbf{w}}(\\mathbf{x}_i), y_i)\\right) \\\\\n              &= \\frac{1}{n} \\sum_{i = 1}^n \\nabla \\ell(f_{\\mathbf{w}}(\\mathbf{x}_i), y_i) \\\\\n              &= \\frac{1}{n} \\sum_{i = 1}^n  \\frac{d\\ell(\\hat{y}_i, y_i)}{d\\hat{y}} \\nabla f_{\\mathbf{w}}(\\mathbf{x}_i) \\tag{multivariate chain rule} \\\\\n              &= \\frac{1}{n} \\sum_{i = 1}^n  \\frac{d\\ell(\\hat{y}_i, y_i)}{d\\hat{y}}  \\mathbf{x}_i \\tag{gradient of a linear function} \\\\\n              &= \\frac{1}{n} \\sum_{i = 1}^n  \\frac{d\\ell(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle, y_i)}{d\\hat{y}} \\mathbf{x}_i \\tag{$\\hat{y}_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle$} \\\\\n\\end{align}\\]\n\n\nImplementation of Gradient Descent for Emperical Risk Minimization\nIn order to implement gradient descent in python, I created a class called LogisticRegression. Within that class, I defined the fit method:\nfit(self, X, y, max_steps)\nAs in the perceptron algorithm, I first made an array \\(\\tilde{\\mathbf{X}} = [\\mathbf{X}, \\mathbf{1}]\\) and initialed the vector \\(\\mathbf{w}\\) with random values from \\(-1\\) to \\(1\\).\nNext, I calculated the gradient of the emperical risk function using logistic loss (described above). I then took a step along this gradient by subrtracting it from the current prediciton vector and multiplying by a scalar alpha:\nself.w -= alpha * self.gradient(self.w, X_, y, self.d_logistic_loss)\nI then repeat the gradient descent step until max_epochs are reached or until the loss show minimaly noticable change, each time adding the emperical risk to loss_history and the accuracy to score_history.\n\n\nImplementation of Stochastic Gradient Descent and Momentum:\nIn order to implement stochastic gradient descent, I created a vector containing numbers 0-n (where n is the number of rows in \\(\\tilde{\\mathbf{X}}\\) using np.order(n) and then randomly shuffled the numbers in the vector.\nI then took batches of length batch_size, calculated the gradient descent on each of those batches, and took a step down the gradient using that calculation.\nFor momentum, I stored the last \\(\\mathbf{w}\\) as prev_w and then calculated the momentum step as 0.8 * (self.w - prev_w). If momentum is set true by the user, then both the momentum step and gradient step are subtracted from \\(\\mathbf{w}\\) in each iteration. Otherwise, only the gradient is subtracted.\n\n\nExperiment 1: Comparison of Regular, Stochastic, and Stochastic with Momentum Gradient Descent\nUsing the make_blobs() fuction, I created non-linearly seperable groups of data with two features. I then implemented all three types of logistic regression. In order to have a fair comparison, I set the alpha and max_epochs for each model to the same number. The default batch_size is set to 10.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom logistic_regression import LogisticRegression\n\nfrom sklearn.datasets import make_blobs, make_circles\n\nnp.random.seed(1234)\n\ndef draw_line(w, x_min, x_max, clr):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = clr)\n\nX, y = make_blobs(n_samples = 200, n_features = 2, centers = [(-1, -1), (1, 1)])\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.5, max_epochs = 1000)\n\nSLR = LogisticRegression()\nSLR.fit_stochastic(X, y, alpha = 0.5, max_epochs = 1000)\n\nMSLR = LogisticRegression()\nMSLR.fit_stochastic(X, y, alpha = 0.5, max_epochs = 1000, momentum = True)\n\nBy plotting each line we see that all three of our models worked pretty well to seperate the sets of data:\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w, -2, 2, \"blue\")\nfig = draw_line(SLR.w, -2, 2, \"orange\")\nfig = draw_line(MSLR.w, -2, 2, \"green\")\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nIn order to compare the learning rates of each algorithm, we can plot the loss over each epoch and see which algorithm converges faster.\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps)+1, LR.loss_history, label = \"gradient\")\n\nnum_steps = len(SLR.loss_history)\nplt.plot(np.arange(num_steps)+1, SLR.loss_history, label = \"stochastic gradient\")\n\nnum_steps = len(MSLR.loss_history)\nplt.plot(np.arange(num_steps)+1, MSLR.loss_history, label = \"stochastic gradient (momentum)\")\n\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\nThis example clealy shows that stochastic gradient descent with momentum converges the quickest of all three algorithms. This graph also shows that by choosing stochastic descent, you may run the risk of noise or “bouncing around” once the algorith nears the solution.\nComparing the scores we also see that the two stochastic gradients converge much quicker than the regular gradient descent:\n\nnum_steps = len(LR.score_history)\nplt.plot(np.arange(num_steps)+1, LR.score_history, label = \"gradient\")\n\nnum_steps = len(SLR.score_history)\nplt.plot(np.arange(num_steps)+1, SLR.score_history, label = \"stochastic gradient\")\n\nnum_steps = len(MSLR.score_history)\nplt.plot(np.arange(num_steps)+1, MSLR.score_history, label = \"stochastic gradient (momentum)\")\n\nxlab = plt.xlabel(\"Ephoch\")\nylab = plt.ylabel(\"Accuracy\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\n\n\nExperiment 2: Effect of Batch Size on Convergence\nThe goal of this experiment is to compare the effect of batch size on convergence in emperical risk minimization. For this experiment, I used the make_blobs() function to make non-linearly seperable data but this time with 5 features. I used stochastic decent and set the batch sizes to different amounts but kept alpha and max_epochs both controled.\n\nX, y = make_blobs(n_samples = 200, n_features = 5, centers = [(-1, -1, -1, -1, -1), (1, 1, 1, 1, 1)])\n\nSLR1 = LogisticRegression()\nSLR1.fit_stochastic(X, y, alpha = 0.5, max_epochs = 1000, batch_size = 50)\n\nSLR2 = LogisticRegression()\nSLR2.fit_stochastic(X, y, alpha = 0.5, max_epochs = 1000, batch_size = 10)\n\nSLR3 = LogisticRegression()\nSLR3.fit_stochastic(X, y, alpha = 0.5, max_epochs = 1000, batch_size = 2)\n\nIn order to compare the learning rates of each algorithm, I again plotted the loss over each epoch and see which algorithm converges faster.\n\nnum_steps = len(SLR1.loss_history)\nplt.plot(np.arange(num_steps)+1, SLR1.loss_history, label = \"batch size = 50\")\n\nnum_steps = len(SLR2.loss_history)\nplt.plot(np.arange(num_steps)+1, SLR2.loss_history, label = \"batch size = 10\")\n\nnum_steps = len(SLR3.loss_history)\nplt.plot(np.arange(num_steps)+1, SLR3.loss_history, label = \"batch size = 2\")\n\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\nThis graph shows us that a smaller batch size converges faster, but it can also lead to much noisier outcomes as it nears the min. Therefore, when choosing a batch size it would be important to have these two things in mind and find a balance between quick convergence and noise.\n\n\nExperiment 3\nIn my final experiment, my goal was to show how the choise of alpha can effect convergence in emperical risk minimization. In this experiment, I again used the make_blobs() function to make non-linearly seperable data but this time with 5 features. This time though, I used regular gradient decent and set the alpha values to different values and kept max_epochs controled.\n\nX, y = make_blobs(n_samples = 200, n_features = 5, centers = [(-1, -1, -1, -1, -1), (1, 1, 1, 1, 1)])\n\nLR1 = LogisticRegression()\nLR1.fit(X, y, alpha = 0.03, max_epochs = 1000)\n\nLR2 = LogisticRegression()\nLR2.fit(X, y, alpha = 3, max_epochs = 1000)\n\nLR3 = LogisticRegression()\nLR3.fit(X, y, alpha = 300, max_epochs = 1000)\n\nIn order to compare the learning rates of each algorithm, I again plotted the loss over each epoch and see which algorithm converges faster, and to perhaps see an example of a alpha that is “too big.”\n\nnum_steps = len(LR1.loss_history)\nplt.plot(np.arange(num_steps)+1, LR1.loss_history, label = \"alpha = 0.03\")\n\nnum_steps = len(LR2.loss_history)\nplt.plot(np.arange(num_steps)+1, LR2.loss_history, label = \"alpha = 3\")\n\nnum_steps = len(LR3.loss_history)\nplt.plot(np.arange(num_steps)+1, LR3.loss_history, label = \"alpha = 300\")\n\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\nlegend = plt.legend()\n\nplt.loglog()\n\n[]\n\n\n\n\n\nThis shows a choise of alpha = 0.03 that is too small it does not converge within the max_epochs, a choise of alpha = 3 that seems to do pretty well, and a choise of alpha = 300 wich also doesnt converge because it “jumps over” the min. This also shows that when choosing alpha it is important to balance convergence speed with accuracy."
  },
  {
    "objectID": "posts/blog5/blog_5.html",
    "href": "posts/blog5/blog_5.html",
    "title": "Blog 5: Auditing Allocative Bias",
    "section": "",
    "text": "GitHub link\nIn this blog post I will be training a model to predict income categories based on certain demographic characteristics and then auditing the model to asses its bias with respect to other demographic characteristics (in this case I will be assesing gender bias)."
  },
  {
    "objectID": "posts/blog5/blog_5.html#bias-measures-discussion",
    "href": "posts/blog5/blog_5.html#bias-measures-discussion",
    "title": "Blog 5: Auditing Allocative Bias",
    "section": "Bias measures discussion",
    "text": "Bias measures discussion\n\nError Rate Balance\nFrom above we see that there is a large discrepancy between the false positive error rates between males and femailes. That means that this model does not pass the error rate balance test.\n\n\nCalibration\nBelow, I calculated and plotted the fraction of respondents whose predicted income was over 50k/year that actually had an income over 50k/year:\n\ndf_male = pd.DataFrame(np.array([y_test_male, y_hat_male]).T, columns=[\"income\", \"income_pred\"])\ndf_fem = pd.DataFrame(np.array([y_test_fem, y_hat_fem]).T, columns=[\"income\", \"income_pred\"])\n\ngb_m = df_male.groupby([\"income_pred\"])[\"income\"].mean()\ngb_f = df_fem.groupby([\"income_pred\"])[\"income\"].mean()\n\ncalibration = [[gb_m[0], gb_f[0]],\n               [gb_m[1], gb_f[1]]]\n\nprint(np.matrix(calibration))\n\ndf_calib = pd.DataFrame(calibration, columns=['Male', 'Female'], index=['False', 'True'])\ndf_calib = df_calib.reset_index().melt('index')\n\n# Plot the bar chart using Seaborn\nsns.set_style(\"whitegrid\")\nax = sns.barplot(x='index', y='value', hue='variable', data=df_calib)\n\n# Set labels and title\nax.set_xlabel('Predicted income >50k/year')\nax.set_ylabel('Percent >50k/year')\nax.set_title('Calibration of Predicted Values by Gender')\n\n# Show the plot\nplt.show()\n\n[[0.26130653 0.16806723]\n [0.83899127 0.64302326]]\n\n\n\n\n\nAs we can see our model is not that well calibrated. The odds that a female who is predicted to have an income above 50k/year actualy has that income is around 64%, whereas for males that number is 84%. Interestingly, this calibration shows somewhat of a bias towards females because women who are predicted to have higher incomes are more likely than men to actually earn less than 50k/year.\n\n\nStatistical Parity\nBelow I calculated the percentage of each gender group that was predicted to have an income over 50k/year.\n\nprint(f\"Male acceptance rate = {(y_hat_male.sum()/len(y_test_male)).round(4)}\")\nprint(f\"Feale acceptance rate = {(y_hat_fem.sum()/len(y_test_fem)).round(4)}\")\n\nMale acceptance rate = 0.5089\nFeale acceptance rate = 0.4454\n\n\nAs we can see, there is also a significant discrepancy for statistical parity that shows bias against women. The algorithm predicts more women to fall into the lower income bracket."
  },
  {
    "objectID": "posts/blog4/blog_4.html",
    "href": "posts/blog4/blog_4.html",
    "title": "Blog 4: Implementing Linear Regression",
    "section": "",
    "text": "In this blog post I will be implement linear regression in two ways, using an analytical method and then using gradient descent. Then, I will perform some experiments on the method to see how the the number of features affects the training validation scores. Lastly, I will compare my linear regression method to scikit-learn’s LASSO method and compare results.\n\nImplementation of Linear Regression with Analytical Solution and Gradient Descent\nTo implement linear regression in Python, I created a class called LinearRegression. Within that class, I defined the fit method, which allows for both analytical and gradient descent approaches.\nThe analytical method uses the formula for the optimal weight vector, as derived in the lecture notes. It involves matrix inversion and several matrix multiplications. Here is the relevant code snippet:\nself.w = np.linalg.inv(P) @ q\nAlternatively, the gradient descent method updates the weight vector iteratively based on the gradient of the loss function. The number of iterations max_epochs and the learning rate alpha can be specified as parameters. Here is the code for gradient descent:\nself.w = np.random.rand(X_.shape[1])\nfor num in range(max_epochs):\n    self.w -= self.gradient(P, q) * alpha\nBy implementing both the analytical solution and gradient descent approaches, this code provides flexibility in choosing the method that best suits the problem at hand. It is likely that he analytical solution is more computationally efficient for small to moderate-sized datasets, while gradient descent is suitable for larger datasets.\n\nCalculating the Gradient of the Loss Function\nTo calculate the gradient of the loss function, I needed to derive the formula for the gradient with respect to the weight vector. In the case of linear regression, the loss function is the mean squared error (MSE).\nThe gradient of the MSE loss function with respect to the weight vector \\(\\mathbf{w}\\) can be calculated as follows:\n\\[\\nabla L(\\mathbf{w}) = 2 \\left(\\mathbf{X}^T \\mathbf{X} \\mathbf{w} - \\mathbf{X}^T \\mathbf{y}\\right)\\]\nIn the code, the gradient is computed using the self.gradient method:\ndef gradient(self, P, q):\n    return 2 * (P @ self.w - q)\nHere, P corresponds to \\(\\mathbf{X}^T \\mathbf{X}\\) and q corresponds to \\(\\mathbf{X}^T \\mathbf{y}\\), both of which are calculated beforehand, because they are constant with each interation.\nBy using this gradient in the gradient descent update step, the weight vector is iteratively adjusted to minimize the loss function and improve the performance of the linear regression model.\n\n\nProof of Concept\nIn order to show my analytic linear regression in action, I used the make_data function from my linear_regression class to create roughly linear data. Then I fit my linear regression using the fit method, setting the method argument to \"analytic\" . Lastly I ploted the data and the resulting line:\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport linear_regression as lin_reg\n\nfrom sklearn.datasets import make_blobs, make_circles\n\nnp.random.seed(123)\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# Create some data\nX_train, y_train, X_val, y_val = lin_reg.make_data(n_train, n_val, p_features, noise)\n\n# Plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n# Fit model\nLR = lin_reg.LinearRegression()\nLR.fit(X_train, y_train, method = \"analytic\")\n\nprint(f\"Training score = {LR.score(lin_reg.pad(X_train), y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(lin_reg.pad(X_val), y_val).round(4)}\")\nprint(f\"w = {LR.w}\")\n\nlin_reg.draw_line(LR.w, 0, 1, \"black\", axarr)\n\nTraining score = 0.5298\nValidation score = 0.6594\nw = [0.83162321 0.52136118]\n\n\n\n\n\nWe also see the same values when run using the \"gradient\" method. Here I also ploted the loss function over each epoch. As we can see it fits quite quickly with a good value for alpha:\n\nnp.random.seed(123)\n\nLR = lin_reg.LinearRegression()\nLR.fit(X_train, y_train, max_epochs = 50, alpha = 0.005, method = \"gradient\")\n\nnum_steps = len(LR1.score_history)\nplt.plot(np.arange(num_steps)+1, LR1.score_history, label = \"gradient\")\n\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Score\")\n\nprint(f\"Training score = {LR.score(lin_reg.pad(X_train), y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(lin_reg.pad(X_val), y_val).round(4)}\")\nprint(f\"w = {LR.w}\")\n\nlegend = plt.legend()\n\nTraining score = 0.5298\nValidation score = 0.6593\nw = [0.83086072 0.521764  ]\n\n\n\n\n\n\n\n\nExperiment: Impact of Increasing Number of Features on Linear Regression\nNext, I cunducted an experiment to understand the impact of increasing the number of features on linear regression performance. The goal was to examine how the training and validation scores change as the number of features used in the model increases, while keeping the number of training points constant.\n\nnp.random.seed(123)\n\nn_train = 200\nn_val = 200\nnoise = 0.2\n\n# Make lists to store the scores with each run\ntrain_scores = []\nval_scores = []\n\np_features_range = range(1, n_train)\n\n# Iterate over different values of p_features\nfor p_features in p_features_range:\n    # Create data with the current p_features value\n    X_train, y_train, X_val, y_val = lin_reg.make_data(n_train, n_val, p_features, noise)\n    \n    #Fit models\n    LR = lin_reg.LinearRegression()\n    LR.fit(X_train, y_train, method=\"analytic\")\n    \n    # Add training and validation scores to list\n    train_score = LR.score(lin_reg.pad(X_train), y_train)\n    val_score = LR.score(lin_reg.pad(X_val), y_val)\n    train_scores.append(train_score)\n    val_scores.append(val_score)\n\n#Print final validation and training score\nprint(\"Scores at p_features = n_train-1:\")\nprint(f\"Training score = {train_scores[-1].round(4)}\")\nprint(f\"Validation score = n_train-1 = {val_scores[-2].round(4)}\")\n\n# Plot the scores\nplt.plot(p_features_range, train_scores, label=\"Training Score\")\nplt.plot(p_features_range, val_scores, label=\"Validation Score\")\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Score\")\nplt.ylim(0.5, max(train_scores)+0.05)\nplt.title(\"Effect of Increasing Number of Features on Score\")\nplt.legend()\nplt.show()\n\nScores at p_features = n_train-1:\nTraining score = 1.0\nValidation score = n_train-1 = 0.4097\n\n\n\n\n\nThe results of the experiment revealed two interesting patterns:\n\nAs the number of features increased, the training score generally improved. This is expected since the model had more information to fit the training data.\nThe validation score initially increased but then started to decrease after reaching a certain number of features. This suggests that the model started to overfit the data as the number of features increased, resulting in a decrease in its ability to generalize to unseen data (the validation set).\n\nThe experiment demonstrates the trade-off between model complexity (captured by the number of features) and overfitting. It highlights the importance of finding an optimal number of features to achieve good generalization.\n\nExperiment with LASSO\nAccording to the blog post instructions, The LASSO algorithm uses a modified loss function with a regularization term:\n\\[L(\\mathbf{w}) = \\lVert \\mathbf{X}\\mathbf{w}- \\mathbf{y} \\rVert_2^2 + \\alpha \\lVert \\mathbf{w}' \\rVert_1\\;\\]\nWhere \\(\\mathbf{w}'\\) is all of the values in \\(\\mathbf{w}\\) except the last.\nI will now attempt the same experiment using LASSO regularization to see the changing effect:\n\nfrom sklearn.linear_model import Lasso\n\nnp.random.seed(123)\n\nn_train = 200\nn_val = 200\nnoise = 0.2\n\n# Make lists to store the scores with each run\ntrain_scores = []\nval_scores = []\n\np_features_range = range(1, n_train)\n\n# Iterate over different values of p_features\nfor p_features in p_features_range:\n    # Create data with the current p_features value\n    X_train, y_train, X_val, y_val = lin_reg.make_data(n_train, n_val, p_features, noise)\n    \n    #Fit models\n    L = Lasso(alpha = 0.0005)\n    L.fit(X_train, y_train)\n    \n    # Add training and validation scores to list\n    train_score = L.score(X_train, y_train)\n    val_score = L.score(X_val, y_val)\n    train_scores.append(train_score)\n    val_scores.append(val_score)\n    \n#Print final validation and training score\nprint(\"Scores at p_features = n_train-1:\")\nprint(f\"Training score = {train_scores[-1].round(4)}\")\nprint(f\"Validation score = {val_scores[-2].round(4)}\")\n\n# Plot the scores\nplt.plot(p_features_range, train_scores, label=\"Training Score\")\nplt.plot(p_features_range, val_scores, label=\"Validation Score\")\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Score\")\nplt.ylim(0.5, max(train_scores)+0.05)\nplt.title(\"Effect of Increasing Number of Features on Score Using Lasso\")\nplt.legend()\nplt.show()\n\nScores at p_features = n_train-1:\nTraining score = 0.9995\nValidation score = 0.832\n\n\n\n\n\nUsing LASSO we can see that there is still overfitting but there at the very extreme end (as p_features nears n_features) the validation score does not stray nearly as far from the training score as it did with my implementation of linear regression."
  },
  {
    "objectID": "posts/blog3/blog_3.html",
    "href": "posts/blog3/blog_3.html",
    "title": "Blog 3: Classifying Palmer Penguins",
    "section": "",
    "text": "In this blog post I will be attempting to classify penguins by finding 3 features that can be used most acurarately catagorize the penguins into 3 species: Chinstrap, Gentoo and Adélie. The Palmer Penguins data set we are using was collected by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network.\n\nLoading and Preparing Data\nIn order to start working with the data, I first have to prepare it so it can be interpreted by the scikit-learn methods. The fist step is to access the data using the pandas library:\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nThis is how that the first few rows of that data look:\n\ntrain.head(3)\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      NaN\n      7.96621\n      -25.69327\n      NaN\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      FEMALE\n      7.63220\n      -25.46569\n      NaN\n    \n    \n      2\n      PAL0910\n      124\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      MALE\n      9.59462\n      -25.42621\n      NaN\n    \n  \n\n\n\n\nNow I have to prepare the qualitative featurs in the data set using pd.get_dummies for Sex and Island and a LabelEncoder for Species. I also removed all irrelevant features:\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nNow if we look at the data we can see that in much better condition to be processed:\n\nX_train.head(3)\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n  \n\n\n\n\n\n\nDisplaying and Discussing Data\nBefore choosing my features, I am going to display and discuss a few of the features in the data set in order to highlight some patterns in the data. First I will use the seaborn library to display the realationsip between flipper length and body mass.\n\nimport seaborn as sns\n\n# Apply the default theme\nsns.set_theme()\n\n# Remove penguin who's sex was not identified\ntrain = train[train[\"Sex\"] != \".\"]\n\n# Create a visualization\nsns.relplot(\n    data=train,\n    x=\"Flipper Length (mm)\", y=\"Body Mass (g)\", col=\"Sex\",\n    hue=\"Species\"\n)\n\n<seaborn.axisgrid.FacetGrid at 0x7fb7956e5a30>\n\n\n\n\n\nFrom this data we can observe a few interesting patterns. The first is that Gentoo penguins are can be easily identified by their body mass and flipper length. The data clearly shows that Gentoo penguins are heavier penguins with larger flippers. Not only are they easily identifiable, the two data sets are linearly seperable (Gento and not Gentoo), given we know the sex of the penguin.\nWe can see also see that Adelie and Chinstrap penguins have very similar body masses and flipper lengths and are therefore not easily identifiable from eachother using those two features.\nThis would mean that in our model, these two factors (body mass and flipper length), although good at identifying Gentoo penguins from non Gentoo penguins, would probably not be good at identifying Adelie from Chinstrap.\nOne other interesting obsrvation we can draw from this graph is that the male penguins tend to be larger then the female penguins.\n\nNow I will take a look at the amount of each species of Penguin on each island.\n\ntrain.groupby([\"Island\", \"Species\"])[[\"Species\"]].count().unstack(fill_value=0).stack()\n\n\n\n\n\n  \n    \n      \n      \n      Species\n    \n    \n      Island\n      Species\n      \n    \n  \n  \n    \n      Biscoe\n      Adelie Penguin (Pygoscelis adeliae)\n      35\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      0\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      100\n    \n    \n      Dream\n      Adelie Penguin (Pygoscelis adeliae)\n      41\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      56\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      0\n    \n    \n      Torgersen\n      Adelie Penguin (Pygoscelis adeliae)\n      42\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      0\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      0\n    \n  \n\n\n\n\nFrom this graph we can see that each species is found on each island in very different amounts. If a penguin is from Biscoe island, it is very unlikely it will be Chinstrap (the data is just a sample so there may be some on the island). Likewise, it would be unlikely to find Gentoo on Dream Island and a Chinstrap or Gentoo on Torgersen island.\nThis also shows that more penguins were surveyed from Biscoe than Dream and many more were surveyed from Dream than Torgersen. This may lead to our clasification algorithm better classifying penguins from Biscoe when compared to those form Dream and Torgersen.\n\n\nChoosing Features\nIn order to choose the three features that could best classify by data, I used sklearn to implement cross validation using linear regression. I first found all possible combinations of one qualitative column and two quantitative columns and then calculated the scores of each combination. Lastly I found the combination with the highest score.\n\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\nall_qual_cols = [\"Clutch Completion_No\", \"Clutch Completion_Yes\", \"Sex_FEMALE\", \"Sex_MALE\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nall_quant_cols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]\n\nscored_cols = []\nLR = LogisticRegression(max_iter = 750)\n\n# Find all posible combinations of one qual. column and two quant. columns\nfor qual in all_qual_cols: \n    qual_cols = [col for col in X_train.columns if qual in col ]\n    for pair in combinations(all_quant_cols, 2):\n        cols = qual_cols + list(pair)\n        # Falculate each combination's scores using linear regression and cross validation\n        scores = cross_val_score(LR, X_train[cols], y_train, cv=5)\n        scored_cols.append([cols, scores.mean()])\n\nbest_score = 0\n\n# Find combination w/ highest score\nfor row in scored_cols:\n    if row[1] > best_score:\n        best_cols = row[0]\n        best_score = row[1]\n    \nprint(best_cols, best_score)\n\n['Island_Dream', 'Culmen Length (mm)', 'Culmen Depth (mm)'] 0.988310708898944\n\n\nAs we can see the Island_Dream, Culmen Length (mm), and Culmen Depth (mm) columns seem to do the best job at classifying the data. The image below by @allison_horst shows what the culmen length and depth refer to.\n\n\n\nCulmen length and depth refers to the length and height of the penguin’s beak\n\n\n\n\nTraining and Ploting Data (Logistic Regression)\nFinally, I will train the logistic regression model on the data. In order to do so, I created a new column called Island_Dream_No which is the opposite of the Island_Dream column. That way I was able desplay all of the data using the plot_regions function defined here.\n\nfrom plot_regions import plot_regions\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Create column that is the opposite of \"Island_Dream\" in order to plot regions\nnot_Island_Dream = X_train[[\"Island_Biscoe\", \"Island_Torgersen\"]].sum(axis=1)\nX_train[\"Island_Dream_No\"] = not_Island_Dream\n\ncols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Island_Dream\", \"Island_Dream_No\"]\n\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\nscore = LR.score(X_train[cols], y_train)\n\nwarnings.filterwarnings(\"default\", category=FutureWarning)\n\nprint(score)\n\nplot_regions(LR, X_train[cols], y_train)\n\n1.0\n\n\n\n\n\nBecause the data is linearly seperable, we are able to reach a score of 1.0. It is also important to note that the data does not seem to be overfit. I think for this reason, linear regression without feature mapping is probably a good choice for a model as it does not “hug” the data. Furthermore, the data seems to be shaped in blobs that can be roughly seperated by linear decision boundries.\n\n\nTest Data\nFinaly, I am going to test to the model on our testing data. I loadd and prepared the data similarly to the training data, ran it thorugh the logistic regression model, then showed the score and plots of this data.\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\nnot_Island_Dream = X_test[[\"Island_Biscoe\", \"Island_Torgersen\"]].sum(axis=1)\nX_test[\"Island_Dream_No\"] = not_Island_Dream\n\nscore = LR.score(X_test[cols], y_test)\nprint(score)\n\nplot_regions(LR, X_test[cols], y_test)\n\n0.9852941176470589\n\n\n\n\n\nWith a score of roughly 98%, this model is very acurate. We can see that the model likely did not overfit to the training data beause it only incorrectly predicted one penguin that was just above the decesion line for it’s correct species."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Weekly Blogs\n\n\n\n\nIn this blog post I will be training a model and auditing it to asses its bias.\n\n\n\n\n\n\nMay 23, 2023\n\n\nAidan McMillan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nFinal Project\n\n\n\n\nBlog post on Timnit Gebru’s virtual visit to Middlebury College.\n\n\n\n\n\n\nMay 22, 2023\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nWeekly Blogs\n\n\n\n\nIn this blog post I will be implementing linear regression.\n\n\n\n\n\n\nMay 16, 2023\n\n\nAidan McMillan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nWeekly Blogs\n\n\n\n\nBlog post on Timnit Gebru’s virtual visit to Middlebury College.\n\n\n\n\n\n\nApr 20, 2023\n\n\nAidan McMillan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nWeekly Blogs\n\n\n\n\nIn this blog post I will be classifing Palmer Penguins based on a select set of features.\n\n\n\n\n\n\nApr 3, 2023\n\n\nAidan McMillan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nWeekly Blogs\n\n\n\n\nIn this blog post I will be implementing logistic regression.\n\n\n\n\n\n\nMar 9, 2023\n\n\nAidan McMillan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nWeekly Blogs\n\n\n\n\nIn this blog post I will be implementing the perceptron algorithm that we learned in the first week of CSCI 0451.\n\n\n\n\n\n\nFeb 25, 2023\n\n\nAidan McMillan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is Aidan McMillan’s CSCI 0451 blog"
  }
]