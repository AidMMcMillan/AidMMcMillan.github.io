[
  {
    "objectID": "posts/blog1/blog_1.html",
    "href": "posts/blog1/blog_1.html",
    "title": "Blog 1: Perceptron Algorithm",
    "section": "",
    "text": "Implementing the Perceptron Algorithm\nIn order to implement the perceptron algorithm in python, I created a perceptron class. Within that class I defined the fit method:\nfit(self, X, y, max_steps)\nBefore iterating through the algorithm, I first made an array \\(\\tilde{\\mathbf{X}} = [\\mathbf{X}, \\mathbf{1}]\\) and initialed the vector \\(\\tilde{\\mathbf{w}}\\) with random values from \\(0-1\\):\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\nself.w = np.random.rand(X_.shape[1])\nAfter initializing \\(\\tilde{\\mathbf{X}}\\) and \\(\\tilde{\\mathbf{w}}\\), I iterated between random vectors in \\(\\tilde{\\mathbf{X}}\\) and updated \\(\\tilde{\\mathbf{w}}\\) using the equation:\n\\[\\tilde{\\mathbf{w}}^{(t+1)} = \\tilde{\\mathbf{w}}^{(t)} + \\mathbb{1}(\\tilde{y}_i \\langle \\tilde{\\mathbf{w}}^{(t)}, \\tilde{\\mathbf{x}}_i\\rangle < 0)\\tilde{y}_i \\tilde{\\mathbf{x}}_i\\]\nIn python that update looks looks like:\nself.w = self.w + (1*((y_[i]*(self.w@X_[i]))<0))*(y_[i]*X_[i])\n\n\nExperiment 1: Linearly Seperable Data\nUsing the make_blobs() fuction, I created two linearly seperable groups of data. I then created an instance of the perceptron class and called the fit method on the data. Plotting both the data and the hyperplane (line) that seperated the data makes it clear that this test resultued in a success:\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom perceptron import Perceptron\n\nfrom sklearn.datasets import make_blobs, make_circles\n\nnp.random.seed(12345)\n\nX, y = make_blobs(n_samples = 100, n_features = 2, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nLooking at the last few accuracy scores we can also see that the perceptron algorithm converged and reached 100% accuracy.\n\nprint(p.history[-10:])\n\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n\n\nThis graph shows the full progression of the acuracy throughout all iterations of the algorithm. We can clealy see that the algorithm converged and it finished before reaching max_steps=1000.\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\nExperiment 2 & 3: Perceptron Algorithm on Non-linearly Seperable Data\nIn experiment two, I used the make_blobs() function but put the centers of the blobs closer together so that they would have overlapping data. As seen in the figure below, the two sets of data are not linearly seperable. We can see that the line seperates the data to some extent but does not completely seperate the data because that would be impossible. This is the line achieved after 1000 iterations of the algorithm.\n\nX, y = make_blobs(n_samples = 100, n_features = 2, centers = [(-1, -1), (1, 1)])\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nBelow we also see that perfect accuracy is not acheived after 1000 iterations. Furthermore, the accuracy also does not consistantly improve with each iteration. It even drops from 97% to 94% in the last two iterations.\n\nprint(p.history[-10:])\n\n[0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.94, 0.94]\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nIn experiment three I created another non linear data set using the make_circles() function. This results in a two sets of data in the shape of a circle, one encompassing the other. From this experiment we also see that it is impossible to seperate the two sets of data with a hyperplane.\nI chose to include this experiment as well in order to highlight the downsides to using perceptrons to seperate data.\nWhile there are cases in which non-linearly seperable data sets are still roughly seperable by a hyperplane (such as experiment 2) there are cases like the one below where a hyperplane would not even be helpful in predicting the labels of the data.\n\nX, y = make_circles(200, shuffle = True, noise = 0.1, factor = 0.5)\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nBelow we see that the acuracy hovers around 50% and does not improve with each iteration. Because we used binary classifiers, a 50% accuracy rate means that this peceptron is just as good as guessing and is therefore not even helpful in classifying the data sets.\n\nprint(p.history[-10:])\n\n[0.475, 0.475, 0.475, 0.5, 0.455, 0.455, 0.455, 0.455, 0.5, 0.49]\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\nEperiment 4: Perceptron Algorithm on 5-dimensional Data\nIn my final experiment, I ran the perceptron algorithm on data with 5 features instead of two. Unfortionally, such data is hard to represent visually so because of the dimensionality. I created the set using make_blobs() with 5 features instead of 2 and centers at (-1, -1, -1, -1, -1) and (1.7, 1.7, 1.7, 1.7, 1.7).\n\nnp.random.seed(123)\n\nX, y = make_blobs(n_samples = 100, n_features = 5, \n                  centers = [(-1, -1, -1, -1, -1), \n                             (1.7, 1.7, 1.7, 1.7, 1.7)])\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\nBelow we see the weights and biases of \\(\\tilde{\\mathbf{w}}\\).\n\nprint(p.w)\n\n[ 3.26420494  0.99757888  1.62033846  1.70676209  0.21630719 -3.72850816]\n\n\nAs shown below, this experiment proved sucessful since the algorithm converged finding a hyperplane that seperated the data with perfect accuracy. This also means our data generated using the make_blobs() function was linearly seperable.\n\nprint(p.history[-10:])\n\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\nRuntime Complexity of Single Iteration of the Perceptron Algorithm\nIn order to calculate the runtime complexity of the one iteration of the perceptron algorithm I must look at the equation used to update \\(\\tilde{\\mathbf{w}}\\):\n\\[\\tilde{\\mathbf{w}}^{(t+1)} = \\tilde{\\mathbf{w}}^{(t)} + \\mathbb{1}(\\tilde{y}_i \\langle \\tilde{\\mathbf{w}}^{(t)}, \\tilde{\\mathbf{x}}_i\\rangle < 0)\\tilde{y}_i \\tilde{\\mathbf{x}}_i\\]\nFirstly, \\(\\tilde{\\mathbf{w}}^{(t)}\\) and \\(\\tilde{\\mathbf{x}}_i\\) are both \\(p+1\\) dimensional vectors, where \\(p\\) is the number of features.\nBy definition, the dot product of two \\(n\\)-dimensionsal vectors \\(a\\) and \\(b\\) is: \\[a_1*b_1 + a_2*b_2 \\space ... + \\space a_n*b_n\\]\nWithin this calculation, \\(n\\) multiplications are performed and \\(n-1\\) additions are performed. Therefore, when \\(\\langle \\tilde{\\mathbf{w}}^{(t)}, \\tilde{\\mathbf{x}}_i\\rangle\\) is calculated, \\(p+1\\) multiplications and \\(p\\) additions are performed.\nTherefore, the complexity of performing the dot product of \\(\\tilde{\\mathbf{w}}^{(t)}\\) and \\(\\tilde{\\mathbf{x}}_i\\) is \\(2p+1=O(p)\\).\nFurthermore, multiplying that dot product by \\(\\tilde{y}_i\\), the \\(<\\) comparison, and the \\(\\mathbb{1}()\\) function are all \\(O(1)\\).\nLastly, the scalar multiplication on \\(\\tilde{\\mathbf{x}}_i\\) requires \\(p+1\\) multilplications and is therefore \\(O(p)\\).\nThat means that the final runtime complexity of a single iteration of the perceptron algorithm is \\(2*O(p) + 3*O(1) = O(p)\\).\nThe runtime complexity of this single operation is therefore not dependent on the number of data points in the set but instead only on the number of features."
  },
  {
    "objectID": "posts/blog2/blog_2.html",
    "href": "posts/blog2/blog_2.html",
    "title": "Blog 2: Optimization of Logistic Regression",
    "section": "",
    "text": "Finding Gradient of Emperical Risk Funtion\nIn order to calculate the gradient descent of the emprical risk fuction, I first had to define the derivitive of the logistic loss function:\n    def d_logistic_loss(self, y_hat, y):\n        return self.sigmoid(y_hat) - y\nThen I defined the gradient as:\n    def gradient(self, X, y, d_loss):\n        return np.mean(np.swapaxes(X, 0, 1)*d_loss(X@self.w, y), axis = 1) \nI used np.swapaxes() to flip the first and second axes of \\(\\tilde{\\mathbf{X}}\\). That allowed me to be able to multiply each \\(\\mathbf{x}_i\\) in \\(\\tilde{\\mathbf{X}}\\) by \\(\\frac{d\\ell(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle, y_i)}{d\\hat{y}}\\). I then found the mean of that entire vector over the second axis (because the axes were flipped) which left me with the gradient vector of the same legth as \\(\\mathbf{w}\\).\nThis gives us the gradient descent of the emprical risk fuction on \\(\\mathbf{w}\\) because of the derivation we saw in class:\n\\[\\begin{align}\n\\nabla L(\\mathbf{w}) &= \\nabla \\left(\\frac{1}{n} \\sum_{i = 1}^n \\ell(f_{\\mathbf{w}}(\\mathbf{x}_i), y_i)\\right) \\\\\n              &= \\frac{1}{n} \\sum_{i = 1}^n \\nabla \\ell(f_{\\mathbf{w}}(\\mathbf{x}_i), y_i) \\\\\n              &= \\frac{1}{n} \\sum_{i = 1}^n  \\frac{d\\ell(\\hat{y}_i, y_i)}{d\\hat{y}} \\nabla f_{\\mathbf{w}}(\\mathbf{x}_i) \\tag{multivariate chain rule} \\\\\n              &= \\frac{1}{n} \\sum_{i = 1}^n  \\frac{d\\ell(\\hat{y}_i, y_i)}{d\\hat{y}}  \\mathbf{x}_i \\tag{gradient of a linear function} \\\\\n              &= \\frac{1}{n} \\sum_{i = 1}^n  \\frac{d\\ell(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle, y_i)}{d\\hat{y}} \\mathbf{x}_i \\tag{$\\hat{y}_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle$} \\\\\n\\end{align}\\]\n\n\nImplementation of Gradient Descent for Emperical Risk Minimization\nIn order to implement gradient descent in python, I created a class called LogisticRegression. Within that class, I defined the fit method:\nfit(self, X, y, max_steps)\nAs in the perceptron algorithm, I first made an array \\(\\tilde{\\mathbf{X}} = [\\mathbf{X}, \\mathbf{1}]\\) and initialed the vector \\(\\mathbf{w}\\) with random values from \\(-1\\) to \\(1\\).\nNext, I calculated the gradient of the emperical risk function using logistic loss (described above). I then took a step along this gradient by subrtracting it from the current prediciton vector and multiplying by a scalar alpha:\nself.w -= alpha * self.gradient(self.w, X_, y, self.d_logistic_loss)\nI then repeat the gradient descent step until max_epochs are reached or until the loss show minimaly noticable change, each time adding the emperical risk to loss_history and the accuracy to score_history.\n\n\nImplementation of Stochastic Gradient Descent and Momentum:\nIn order to implement stochastic gradient descent, I created a vector containing numbers 0-n (where n is the number of rows in \\(\\tilde{\\mathbf{X}}\\) using np.order(n) and then randomly shuffled the numbers in the vector.\nI then took batches of length batch_size, calculated the gradient descent on each of those batches, and took a step down the gradient using that calculation.\nFor momentum, I stored the last \\(\\mathbf{w}\\) as prev_w and then calculated the momentum step as 0.8 * (self.w - prev_w). If momentum is set true by the user, then both the momentum step and gradient step are subtracted from \\(\\mathbf{w}\\) in each iteration. Otherwise, only the gradient is subtracted.\n\n\nExperiment 1: Comparison of Regular, Stochastic, and Stochastic with Momentum Gradient Descent\nUsing the make_blobs() fuction, I created non-linearly seperable groups of data with two features. I then implemented all three types of logistic regression. In order to have a fair comparison, I set the alpha and max_epochs for each model to the same number. The default batch_size is set to 10.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom logistic_regression import LogisticRegression\n\nfrom sklearn.datasets import make_blobs, make_circles\n\nnp.random.seed(1234)\n\ndef draw_line(w, x_min, x_max, clr):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = clr)\n\nX, y = make_blobs(n_samples = 200, n_features = 2, centers = [(-1, -1), (1, 1)])\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.5, max_epochs = 1000)\n\nSLR = LogisticRegression()\nSLR.fit_stochastic(X, y, alpha = 0.5, max_epochs = 1000)\n\nMSLR = LogisticRegression()\nMSLR.fit_stochastic(X, y, alpha = 0.5, max_epochs = 1000, momentum = True)\n\nBy plotting each line we see that all three of our models worked pretty well to seperate the sets of data:\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w, -2, 2, \"blue\")\nfig = draw_line(SLR.w, -2, 2, \"orange\")\nfig = draw_line(MSLR.w, -2, 2, \"green\")\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nIn order to compare the learning rates of each algorithm, we can plot the loss over each epoch and see which algorithm converges faster.\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps)+1, LR.loss_history, label = \"gradient\")\n\nnum_steps = len(SLR.loss_history)\nplt.plot(np.arange(num_steps)+1, SLR.loss_history, label = \"stochastic gradient\")\n\nnum_steps = len(MSLR.loss_history)\nplt.plot(np.arange(num_steps)+1, MSLR.loss_history, label = \"stochastic gradient (momentum)\")\n\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\nThis example clealy shows that stochastic gradient descent with momentum converges the quickest of all three algorithms. This graph also shows that by choosing stochastic descent, you may run the risk of noise or “bouncing around” once the algorith nears the solution.\nComparing the scores we also see that the two stochastic gradients converge much quicker than the regular gradient descent:\n\nnum_steps = len(LR.score_history)\nplt.plot(np.arange(num_steps)+1, LR.score_history, label = \"gradient\")\n\nnum_steps = len(SLR.score_history)\nplt.plot(np.arange(num_steps)+1, SLR.score_history, label = \"stochastic gradient\")\n\nnum_steps = len(MSLR.score_history)\nplt.plot(np.arange(num_steps)+1, MSLR.score_history, label = \"stochastic gradient (momentum)\")\n\nxlab = plt.xlabel(\"Ephoch\")\nylab = plt.ylabel(\"Accuracy\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\n\n\nExperiment 2: Effect of Batch Size on Convergence\nThe goal of this experiment is to compare the effect of batch size on convergence in emperical risk minimization. For this experiment, I used the make_blobs() function to make non-linearly seperable data but this time with 5 features. I used stochastic decent and set the batch sizes to different amounts but kept alpha and max_epochs both controled.\n\nX, y = make_blobs(n_samples = 200, n_features = 5, centers = [(-1, -1, -1, -1, -1), (1, 1, 1, 1, 1)])\n\nSLR1 = LogisticRegression()\nSLR1.fit_stochastic(X, y, alpha = 0.5, max_epochs = 1000, batch_size = 50)\n\nSLR2 = LogisticRegression()\nSLR2.fit_stochastic(X, y, alpha = 0.5, max_epochs = 1000, batch_size = 10)\n\nSLR3 = LogisticRegression()\nSLR3.fit_stochastic(X, y, alpha = 0.5, max_epochs = 1000, batch_size = 2)\n\nIn order to compare the learning rates of each algorithm, I again plotted the loss over each epoch and see which algorithm converges faster.\n\nnum_steps = len(SLR1.loss_history)\nplt.plot(np.arange(num_steps)+1, SLR1.loss_history, label = \"batch size = 50\")\n\nnum_steps = len(SLR2.loss_history)\nplt.plot(np.arange(num_steps)+1, SLR2.loss_history, label = \"batch size = 10\")\n\nnum_steps = len(SLR3.loss_history)\nplt.plot(np.arange(num_steps)+1, SLR3.loss_history, label = \"batch size = 2\")\n\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\nThis graph shows us that a smaller batch size converges faster, but it can also lead to much noisier outcomes as it nears the min. Therefore, when choosing a batch size it would be important to have these two things in mind and find a balance between quick convergence and noise.\n\n\nExperiment 3\nIn my final experiment, my goal was to show how the choise of alpha can effect convergence in emperical risk minimization. In this experiment, I again used the make_blobs() function to make non-linearly seperable data but this time with 5 features. This time though, I used regular gradient decent and set the alpha values to different values and kept max_epochs controled.\n\nX, y = make_blobs(n_samples = 200, n_features = 5, centers = [(-1, -1, -1, -1, -1), (1, 1, 1, 1, 1)])\n\nLR1 = LogisticRegression()\nLR1.fit(X, y, alpha = 0.03, max_epochs = 1000)\n\nLR2 = LogisticRegression()\nLR2.fit(X, y, alpha = 3, max_epochs = 1000)\n\nLR3 = LogisticRegression()\nLR3.fit(X, y, alpha = 300, max_epochs = 1000)\n\nIn order to compare the learning rates of each algorithm, I again plotted the loss over each epoch and see which algorithm converges faster, and to perhaps see an example of a alpha that is “too big.”\n\nnum_steps = len(LR1.loss_history)\nplt.plot(np.arange(num_steps)+1, LR1.loss_history, label = \"alpha = 0.03\")\n\nnum_steps = len(LR2.loss_history)\nplt.plot(np.arange(num_steps)+1, LR2.loss_history, label = \"alpha = 3\")\n\nnum_steps = len(LR3.loss_history)\nplt.plot(np.arange(num_steps)+1, LR3.loss_history, label = \"alpha = 300\")\n\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\nlegend = plt.legend()\n\nplt.loglog()\n\n[]\n\n\n\n\n\nThis shows a choise of alpha = 0.03 that is too small it does not converge within the max_epochs, a choise of alpha = 3 that seems to do pretty well, and a choise of alpha = 300 wich also doesnt converge because it “jumps over” the min. This also shows that when choosing alpha it is important to balance convergence speed with accuracy."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Weekly Blogs\n\n\n\n\nIn this blog post I will be implementing logistic regression.\n\n\n\n\n\n\nMar 9, 2023\n\n\nAidan McMillan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nWeekly Blogs\n\n\n\n\nIn this blog post I will be implementing the perceptron algorithm that we learned in the first week of CSCI 0451.\n\n\n\n\n\n\nFeb 25, 2023\n\n\nAidan McMillan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is Aidan McMillan’s CSCI 0451 blog"
  }
]