[
  {
    "objectID": "posts/blog1/blog_1.html",
    "href": "posts/blog1/blog_1.html",
    "title": "Blog 1: Perceptron Algorithm",
    "section": "",
    "text": "Implementing the Perceptron Algorithm\nIn order to implement the perceptron algorithm in python, I created a perceptron class. Within that class I defined the fit method:\nfit(self, X, y, max_steps)\nBefore iterating through the algorithm, I first made an array \\(\\tilde{\\mathbf{X}} = [\\mathbf{X}, \\mathbf{1}]\\) and initialed the vector \\(\\tilde{\\mathbf{w}}\\) with random values from \\(0-1\\):\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\nself.w = np.random.rand(X_.shape[1])\nAfter initializing \\(\\tilde{\\mathbf{X}}\\) and \\(\\tilde{\\mathbf{w}}\\), I iterated between random vectors in \\(\\tilde{\\mathbf{X}}\\) and updated \\(\\tilde{\\mathbf{w}}\\) using the equation:\n\\[\\tilde{\\mathbf{w}}^{(t+1)} = \\tilde{\\mathbf{w}}^{(t)} + \\mathbb{1}(\\tilde{y}_i \\langle \\tilde{\\mathbf{w}}^{(t)}, \\tilde{\\mathbf{x}}_i\\rangle < 0)\\tilde{y}_i \\tilde{\\mathbf{x}}_i\\]\nIn python that update looks looks like:\nself.w = self.w + (1*((y_[i]*(self.w@X_[i]))<0))*(y_[i]*X_[i])\n\n\nExperiment 1: Linearly Seperable Data\nUsing the make_blobs() fuction, I created two linearly seperable groups of data. I then created an instance of the perceptron class and called the fit method on the data. Plotting both the data and the hyperplane (line) that seperated the data makes it clear that this test resultued in a success:\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom perceptron import Perceptron\n\nfrom sklearn.datasets import make_blobs, make_circles\n\nnp.random.seed(12345)\n\nX, y = make_blobs(n_samples = 100, n_features = 2, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nLooking at the last few accuracy scores we can also see that the perceptron algorithm converged and reached 100% accuracy.\n\nprint(p.history[-10:])\n\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n\n\nThis graph shows the full progression of the acuracy throughout all iterations of the algorithm. We can clealy see that the algorithm converged and it finished before reaching max_steps=1000.\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\nExperiment 2 & 3: Perceptron Algorithm on Non-linearly Seperable Data\nIn experiment two, I used the make_blobs() function but put the centers of the blobs closer together so that they would have overlapping data. As seen in the figure below, the two sets of data are not linearly seperable. We can see that the line seperates the data to some extent but does not completely seperate the data because that would be impossible. This is the line achieved after 1000 iterations of the algorithm.\n\nX, y = make_blobs(n_samples = 100, n_features = 2, centers = [(-1, -1), (1, 1)])\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nBelow we also see that perfect accuracy is not acheived after 1000 iterations. Furthermore, the accuracy also does not consistantly improve with each iteration. It even drops from 97% to 94% in the last two iterations.\n\nprint(p.history[-10:])\n\n[0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.94, 0.94]\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nIn experiment three I created another non linear data set using the make_circles() function. This results in a two sets of data in the shape of a circle, one encompassing the other. From this experiment we also see that it is impossible to seperate the two sets of data with a hyperplane.\nI chose to include this experiment as well in order to highlight the downsides to using perceptrons to seperate data.\nWhile there are cases in which non-linearly seperable data sets are still roughly seperable by a hyperplane (such as experiment 2) there are cases like the one below where a hyperplane would not even be helpful in predicting the labels of the data.\n\nX, y = make_circles(200, shuffle = True, noise = 0.1, factor = 0.5)\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nBelow we see that the acuracy hovers around 50% and does not improve with each iteration. Because we used binary classifiers, a 50% accuracy rate means that this peceptron is just as good as guessing and is therefore not even helpful in classifying the data sets.\n\nprint(p.history[-10:])\n\n[0.475, 0.475, 0.475, 0.5, 0.455, 0.455, 0.455, 0.455, 0.5, 0.49]\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\nEperiment 4: Perceptron Algorithm on 5-dimensional Data\nIn my final experiment, I ran the perceptron algorithm on data with 5 features instead of two. Unfortionally, such data is hard to represent visually so because of the dimensionality. I created the set using make_blobs() with 5 features instead of 2 and centers at (-1, -1, -1, -1, -1) and (1.7, 1.7, 1.7, 1.7, 1.7).\n\nnp.random.seed(123)\n\nX, y = make_blobs(n_samples = 100, n_features = 5, \n                  centers = [(-1, -1, -1, -1, -1), \n                             (1.7, 1.7, 1.7, 1.7, 1.7)])\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\nBelow we see the weights and biases of \\(\\tilde{\\mathbf{w}}\\).\n\nprint(p.w)\n\n[ 3.26420494  0.99757888  1.62033846  1.70676209  0.21630719 -3.72850816]\n\n\nAs shown below, this experiment proved sucessful since the algorithm converged finding a hyperplane that seperated the data with perfect accuracy. This also means our data generated using the make_blobs() function was linearly seperable.\n\nprint(p.history[-10:])\n\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\nRuntime Complexity of Single Iteration of the Perceptron Algorithm\nIn order to calculate the runtime complexity of the one iteration of the perceptron algorithm I must look at the equation used to update \\(\\tilde{\\mathbf{w}}\\):\n\\[\\tilde{\\mathbf{w}}^{(t+1)} = \\tilde{\\mathbf{w}}^{(t)} + \\mathbb{1}(\\tilde{y}_i \\langle \\tilde{\\mathbf{w}}^{(t)}, \\tilde{\\mathbf{x}}_i\\rangle < 0)\\tilde{y}_i \\tilde{\\mathbf{x}}_i\\]\nFirstly, \\(\\tilde{\\mathbf{w}}^{(t)}\\) and \\(\\tilde{\\mathbf{x}}_i\\) are both \\(p+1\\) dimensional vectors, where \\(p\\) is the number of features.\nBy definition, the dot product of two \\(n\\)-dimensionsal vectors \\(a\\) and \\(b\\) is: \\[a_1*b_1 + a_2*b_2 \\space ... + \\space a_n*b_n\\]\nWithin this calculation, \\(n\\) multiplications are performed and \\(n-1\\) additions are performed. Therefore, when \\(\\langle \\tilde{\\mathbf{w}}^{(t)}, \\tilde{\\mathbf{x}}_i\\rangle\\) is calculated, \\(p+1\\) multiplications and \\(p\\) additions are performed.\nTherefore, the complexity of performing the dot product of \\(\\tilde{\\mathbf{w}}^{(t)}\\) and \\(\\tilde{\\mathbf{x}}_i\\) is \\(2p+1=O(p)\\).\nFurthermore, multiplying that dot product by \\(\\tilde{y}_i\\), the \\(<\\) comparison, and the \\(\\mathbb{1}()\\) function are all \\(O(1)\\).\nLastly, the scalar multiplication on \\(\\tilde{\\mathbf{x}}_i\\) requires \\(p+1\\) multilplications and is therefore \\(O(p)\\).\nThat means that the final runtime complexity of a single iteration of the perceptron algorithm is \\(2*O(p) + 3*O(1) = O(p)\\).\nThe runtime complexity of this single operation is therefore not dependent on the number of data points in the set but instead only on the number of features."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Weekly Blogs\n\n\n\n\nIn this blog post I will be implementing the perceptron algorithm that we learned in the first week of CSCI 0451.\n\n\n\n\n\n\nFeb 25, 2023\n\n\nAidan McMillan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is Aidan McMillan’s CSCI 0451 blog"
  }
]