[
  {
    "objectID": "posts/blog1/blog_1.html",
    "href": "posts/blog1/blog_1.html",
    "title": "Blog 1: Perceptron Algorithm",
    "section": "",
    "text": "Implementing the Perceptron Algorithm\nIn order to implement the perceptron algorithm in python, I created a perceptron class. Within that class I defined the fit method:\nfit(self, X, y, max_steps)\nBefore iterating through the algorithm, I first made an array \\(\\tilde{\\mathbf{X}} = [\\mathbf{X}, \\mathbf{1}]\\) and initialed the vector \\(\\tilde{\\mathbf{w}}\\) with random values from \\(0-1\\):\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\nself.w = np.random.rand(X_.shape[1])\nAfter initializing \\(\\tilde{\\mathbf{X}}\\) and \\(\\tilde{\\mathbf{w}}\\), I iterated between random vectors in \\(\\tilde{\\mathbf{X}}\\) and updated \\(\\tilde{\\mathbf{w}}\\) using the equation:\n\\[\\tilde{\\mathbf{w}}^{(t+1)} = \\tilde{\\mathbf{w}}^{(t)} + \\mathbb{1}(\\tilde{y}_i \\langle \\tilde{\\mathbf{w}}^{(t)}, \\tilde{\\mathbf{x}}_i\\rangle < 0)\\tilde{y}_i \\tilde{\\mathbf{x}}_i\\]\nIn python that update looks looks like:\nself.w = self.w + (1*((y_[i]*(self.w@X_[i]))<0))*(y_[i]*X_[i])\n\n\nExperiment 1: Linearly Seperable Data\nUsing the make_blobs() fuction, I created two linearly seperable groups of data. I then created an instance of the perceptron class and called the fit method on the data. Plotting both the data and the hyperplane (line) that seperated the data makes it clear that this test resultued in a success:\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom perceptron import Perceptron\n\nfrom sklearn.datasets import make_blobs, make_circles\n\nnp.random.seed(12345)\n\nX, y = make_blobs(n_samples = 100, n_features = 2, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nLooking at the last few accuracy scores we can also see that the perceptron algorithm converged and reached 100% accuracy.\n\nprint(p.history[-10:])\n\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n\n\nThis graph shows the full progression of the acuracy throughout all iterations of the algorithm. We can clealy see that the algorithm converged and it finished before reaching max_steps=1000.\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\nExperiment 2 & 3: Perceptron Algorithm on Non-linearly Seperable Data\nIn experiment two, I used the make_blobs() function but put the centers of the blobs closer together so that they would have overlapping data. As seen in the figure below, the two sets of data are not linearly seperable. We can see that the line seperates the data to some extent but does not completely seperate the data because that would be impossible. This is the line achieved after 1000 iterations of the algorithm.\n\nX, y = make_blobs(n_samples = 100, n_features = 2, centers = [(-1, -1), (1, 1)])\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nBelow we also see that perfect accuracy is not acheived after 1000 iterations. Furthermore, the accuracy also does not consistantly improve with each iteration. It even drops from 97% to 94% in the last two iterations.\n\nprint(p.history[-10:])\n\n[0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.94, 0.94]\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nIn experiment three I created another non linear data set using the make_circles() function. This results in a two sets of data in the shape of a circle, one encompassing the other. From this experiment we also see that it is impossible to seperate the two sets of data with a hyperplane.\nI chose to include this experiment as well in order to highlight the downsides to using perceptrons to seperate data.\nWhile there are cases in which non-linearly seperable data sets are still roughly seperable by a hyperplane (such as experiment 2) there are cases like the one below where a hyperplane would not even be helpful in predicting the labels of the data.\n\nX, y = make_circles(200, shuffle = True, noise = 0.1, factor = 0.5)\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nBelow we see that the acuracy hovers around 50% and does not improve with each iteration. Because we used binary classifiers, a 50% accuracy rate means that this peceptron is just as good as guessing and is therefore not even helpful in classifying the data sets.\n\nprint(p.history[-10:])\n\n[0.475, 0.475, 0.475, 0.5, 0.455, 0.455, 0.455, 0.455, 0.5, 0.49]\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\nEperiment 4: Perceptron Algorithm on 5-dimensional Data\nIn my final experiment, I ran the perceptron algorithm on data with 5 features instead of two. Unfortionally, such data is hard to represent visually so because of the dimensionality. I created the set using make_blobs() with 5 features instead of 2 and centers at (-1, -1, -1, -1, -1) and (1.7, 1.7, 1.7, 1.7, 1.7).\n\nnp.random.seed(123)\n\nX, y = make_blobs(n_samples = 100, n_features = 5, \n                  centers = [(-1, -1, -1, -1, -1), \n                             (1.7, 1.7, 1.7, 1.7, 1.7)])\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\nBelow we see the weights and biases of \\(\\tilde{\\mathbf{w}}\\).\n\nprint(p.w)\n\n[ 3.26420494  0.99757888  1.62033846  1.70676209  0.21630719 -3.72850816]\n\n\nAs shown below, this experiment proved sucessful since the algorithm converged finding a hyperplane that seperated the data with perfect accuracy. This also means our data generated using the make_blobs() function was linearly seperable.\n\nprint(p.history[-10:])\n\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\nRuntime Complexity of Single Iteration of the Perceptron Algorithm\nIn order to calculate the runtime complexity of the one iteration of the perceptron algorithm I must look at the equation used to update \\(\\tilde{\\mathbf{w}}\\):\n\\[\\tilde{\\mathbf{w}}^{(t+1)} = \\tilde{\\mathbf{w}}^{(t)} + \\mathbb{1}(\\tilde{y}_i \\langle \\tilde{\\mathbf{w}}^{(t)}, \\tilde{\\mathbf{x}}_i\\rangle < 0)\\tilde{y}_i \\tilde{\\mathbf{x}}_i\\]\nFirstly, \\(\\tilde{\\mathbf{w}}^{(t)}\\) and \\(\\tilde{\\mathbf{x}}_i\\) are both \\(p+1\\) dimensional vectors, where \\(p\\) is the number of features.\nBy definition, the dot product of two \\(n\\)-dimensionsal vectors \\(a\\) and \\(b\\) is: \\[a_1*b_1 + a_2*b_2 \\space ... + \\space a_n*b_n\\]\nWithin this calculation, \\(n\\) multiplications are performed and \\(n-1\\) additions are performed. Therefore, when \\(\\langle \\tilde{\\mathbf{w}}^{(t)}, \\tilde{\\mathbf{x}}_i\\rangle\\) is calculated, \\(p+1\\) multiplications and \\(p\\) additions are performed.\nTherefore, the complexity of performing the dot product of \\(\\tilde{\\mathbf{w}}^{(t)}\\) and \\(\\tilde{\\mathbf{x}}_i\\) is \\(2p+1=O(p)\\).\nFurthermore, multiplying that dot product by \\(\\tilde{y}_i\\), the \\(<\\) comparison, and the \\(\\mathbb{1}()\\) function are all \\(O(1)\\).\nLastly, the scalar multiplication on \\(\\tilde{\\mathbf{x}}_i\\) requires \\(p+1\\) multilplications and is therefore \\(O(p)\\).\nThat means that the final runtime complexity of a single iteration of the perceptron algorithm is \\(2*O(p) + 3*O(1) = O(p)\\).\nThe runtime complexity of this single operation is therefore not dependent on the number of data points in the set but instead only on the number of features."
  },
  {
    "objectID": "posts/blog2/blog_2.html",
    "href": "posts/blog2/blog_2.html",
    "title": "Blog 2: Optimization of Logistic Regression",
    "section": "",
    "text": "Finding Gradient of Emperical Risk Funtion\nIn order to calculate the gradient descent of the emprical risk fuction, I first had to define the derivitive of the logistic loss function:\n    def d_logistic_loss(self, y_hat, y):\n        return self.sigmoid(y_hat) - y\nThen I defined the gradient as:\n    def gradient(self, X, y, d_loss):\n        return np.mean(np.swapaxes(X, 0, 1)*d_loss(X@self.w, y), axis = 1) \nI used np.swapaxes() to flip the first and second axes of \\(\\tilde{\\mathbf{X}}\\). That allowed me to be able to multiply each \\(\\mathbf{x}_i\\) in \\(\\tilde{\\mathbf{X}}\\) by \\(\\frac{d\\ell(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle, y_i)}{d\\hat{y}}\\). I then found the mean of that entire vector over the second axis (because the axes were flipped) which left me with the gradient vector of the same legth as \\(\\mathbf{w}\\).\nThis gives us the gradient descent of the emprical risk fuction on \\(\\mathbf{w}\\) because of the derivation we saw in class:\n\\[\\begin{align}\n\\nabla L(\\mathbf{w}) &= \\nabla \\left(\\frac{1}{n} \\sum_{i = 1}^n \\ell(f_{\\mathbf{w}}(\\mathbf{x}_i), y_i)\\right) \\\\\n              &= \\frac{1}{n} \\sum_{i = 1}^n \\nabla \\ell(f_{\\mathbf{w}}(\\mathbf{x}_i), y_i) \\\\\n              &= \\frac{1}{n} \\sum_{i = 1}^n  \\frac{d\\ell(\\hat{y}_i, y_i)}{d\\hat{y}} \\nabla f_{\\mathbf{w}}(\\mathbf{x}_i) \\tag{multivariate chain rule} \\\\\n              &= \\frac{1}{n} \\sum_{i = 1}^n  \\frac{d\\ell(\\hat{y}_i, y_i)}{d\\hat{y}}  \\mathbf{x}_i \\tag{gradient of a linear function} \\\\\n              &= \\frac{1}{n} \\sum_{i = 1}^n  \\frac{d\\ell(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle, y_i)}{d\\hat{y}} \\mathbf{x}_i \\tag{$\\hat{y}_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle$} \\\\\n\\end{align}\\]\n\n\nImplementation of Gradient Descent for Emperical Risk Minimization\nIn order to implement gradient descent in python, I created a class called LogisticRegression. Within that class, I defined the fit method:\nfit(self, X, y, max_steps)\nAs in the perceptron algorithm, I first made an array \\(\\tilde{\\mathbf{X}} = [\\mathbf{X}, \\mathbf{1}]\\) and initialed the vector \\(\\mathbf{w}\\) with random values from \\(-1\\) to \\(1\\).\nNext, I calculated the gradient of the emperical risk function using logistic loss (described above). I then took a step along this gradient by subrtracting it from the current prediciton vector and multiplying by a scalar alpha:\nself.w -= alpha * self.gradient(self.w, X_, y, self.d_logistic_loss)\nI then repeat the gradient descent step until max_epochs are reached or until the loss show minimaly noticable change, each time adding the emperical risk to loss_history and the accuracy to score_history.\n\n\nImplementation of Stochastic Gradient Descent and Momentum:\nIn order to implement stochastic gradient descent, I created a vector containing numbers 0-n (where n is the number of rows in \\(\\tilde{\\mathbf{X}}\\) using np.order(n) and then randomly shuffled the numbers in the vector.\nI then took batches of length batch_size, calculated the gradient descent on each of those batches, and took a step down the gradient using that calculation.\nFor momentum, I stored the last \\(\\mathbf{w}\\) as prev_w and then calculated the momentum step as 0.8 * (self.w - prev_w). If momentum is set true by the user, then both the momentum step and gradient step are subtracted from \\(\\mathbf{w}\\) in each iteration. Otherwise, only the gradient is subtracted.\n\n\nExperiment 1: Comparison of Regular, Stochastic, and Stochastic with Momentum Gradient Descent\nUsing the make_blobs() fuction, I created non-linearly seperable groups of data with two features. I then implemented all three types of logistic regression. In order to have a fair comparison, I set the alpha and max_epochs for each model to the same number. The default batch_size is set to 10.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom logistic_regression import LogisticRegression\n\nfrom sklearn.datasets import make_blobs, make_circles\n\nnp.random.seed(1234)\n\ndef draw_line(w, x_min, x_max, clr):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = clr)\n\nX, y = make_blobs(n_samples = 200, n_features = 2, centers = [(-1, -1), (1, 1)])\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.5, max_epochs = 1000)\n\nSLR = LogisticRegression()\nSLR.fit_stochastic(X, y, alpha = 0.5, max_epochs = 1000)\n\nMSLR = LogisticRegression()\nMSLR.fit_stochastic(X, y, alpha = 0.5, max_epochs = 1000, momentum = True)\n\nBy plotting each line we see that all three of our models worked pretty well to seperate the sets of data:\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w, -2, 2, \"blue\")\nfig = draw_line(SLR.w, -2, 2, \"orange\")\nfig = draw_line(MSLR.w, -2, 2, \"green\")\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nIn order to compare the learning rates of each algorithm, we can plot the loss over each epoch and see which algorithm converges faster.\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps)+1, LR.loss_history, label = \"gradient\")\n\nnum_steps = len(SLR.loss_history)\nplt.plot(np.arange(num_steps)+1, SLR.loss_history, label = \"stochastic gradient\")\n\nnum_steps = len(MSLR.loss_history)\nplt.plot(np.arange(num_steps)+1, MSLR.loss_history, label = \"stochastic gradient (momentum)\")\n\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\nThis example clealy shows that stochastic gradient descent with momentum converges the quickest of all three algorithms. This graph also shows that by choosing stochastic descent, you may run the risk of noise or “bouncing around” once the algorith nears the solution.\nComparing the scores we also see that the two stochastic gradients converge much quicker than the regular gradient descent:\n\nnum_steps = len(LR.score_history)\nplt.plot(np.arange(num_steps)+1, LR.score_history, label = \"gradient\")\n\nnum_steps = len(SLR.score_history)\nplt.plot(np.arange(num_steps)+1, SLR.score_history, label = \"stochastic gradient\")\n\nnum_steps = len(MSLR.score_history)\nplt.plot(np.arange(num_steps)+1, MSLR.score_history, label = \"stochastic gradient (momentum)\")\n\nxlab = plt.xlabel(\"Ephoch\")\nylab = plt.ylabel(\"Accuracy\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\n\n\nExperiment 2: Effect of Batch Size on Convergence\nThe goal of this experiment is to compare the effect of batch size on convergence in emperical risk minimization. For this experiment, I used the make_blobs() function to make non-linearly seperable data but this time with 5 features. I used stochastic decent and set the batch sizes to different amounts but kept alpha and max_epochs both controled.\n\nX, y = make_blobs(n_samples = 200, n_features = 5, centers = [(-1, -1, -1, -1, -1), (1, 1, 1, 1, 1)])\n\nSLR1 = LogisticRegression()\nSLR1.fit_stochastic(X, y, alpha = 0.5, max_epochs = 1000, batch_size = 50)\n\nSLR2 = LogisticRegression()\nSLR2.fit_stochastic(X, y, alpha = 0.5, max_epochs = 1000, batch_size = 10)\n\nSLR3 = LogisticRegression()\nSLR3.fit_stochastic(X, y, alpha = 0.5, max_epochs = 1000, batch_size = 2)\n\nIn order to compare the learning rates of each algorithm, I again plotted the loss over each epoch and see which algorithm converges faster.\n\nnum_steps = len(SLR1.loss_history)\nplt.plot(np.arange(num_steps)+1, SLR1.loss_history, label = \"batch size = 50\")\n\nnum_steps = len(SLR2.loss_history)\nplt.plot(np.arange(num_steps)+1, SLR2.loss_history, label = \"batch size = 10\")\n\nnum_steps = len(SLR3.loss_history)\nplt.plot(np.arange(num_steps)+1, SLR3.loss_history, label = \"batch size = 2\")\n\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\nThis graph shows us that a smaller batch size converges faster, but it can also lead to much noisier outcomes as it nears the min. Therefore, when choosing a batch size it would be important to have these two things in mind and find a balance between quick convergence and noise.\n\n\nExperiment 3\nIn my final experiment, my goal was to show how the choise of alpha can effect convergence in emperical risk minimization. In this experiment, I again used the make_blobs() function to make non-linearly seperable data but this time with 5 features. This time though, I used regular gradient decent and set the alpha values to different values and kept max_epochs controled.\n\nX, y = make_blobs(n_samples = 200, n_features = 5, centers = [(-1, -1, -1, -1, -1), (1, 1, 1, 1, 1)])\n\nLR1 = LogisticRegression()\nLR1.fit(X, y, alpha = 0.03, max_epochs = 1000)\n\nLR2 = LogisticRegression()\nLR2.fit(X, y, alpha = 3, max_epochs = 1000)\n\nLR3 = LogisticRegression()\nLR3.fit(X, y, alpha = 300, max_epochs = 1000)\n\nIn order to compare the learning rates of each algorithm, I again plotted the loss over each epoch and see which algorithm converges faster, and to perhaps see an example of a alpha that is “too big.”\n\nnum_steps = len(LR1.loss_history)\nplt.plot(np.arange(num_steps)+1, LR1.loss_history, label = \"alpha = 0.03\")\n\nnum_steps = len(LR2.loss_history)\nplt.plot(np.arange(num_steps)+1, LR2.loss_history, label = \"alpha = 3\")\n\nnum_steps = len(LR3.loss_history)\nplt.plot(np.arange(num_steps)+1, LR3.loss_history, label = \"alpha = 300\")\n\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\nlegend = plt.legend()\n\nplt.loglog()\n\n[]\n\n\n\n\n\nThis shows a choise of alpha = 0.03 that is too small it does not converge within the max_epochs, a choise of alpha = 3 that seems to do pretty well, and a choise of alpha = 300 wich also doesnt converge because it “jumps over” the min. This also shows that when choosing alpha it is important to balance convergence speed with accuracy."
  },
  {
    "objectID": "posts/blog3/blog_3.html",
    "href": "posts/blog3/blog_3.html",
    "title": "Blog 3: Classifying Palmer Penguins",
    "section": "",
    "text": "In this blog post I will be attempting to classify penguins by finding 3 features that can be used most acurarately catagorize the penguins into 3 species: Chinstrap, Gentoo and Adélie. The Palmer Penguins data set we are using was collected by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network.\n\nLoading and Preparing Data\nIn order to start working with the data, I first have to prepare it so it can be interpreted by the scikit-learn methods. The fist step is to access the data using the pandas library:\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nThis is how that the first few rows of that data look:\n\ntrain.head(3)\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      NaN\n      7.96621\n      -25.69327\n      NaN\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      FEMALE\n      7.63220\n      -25.46569\n      NaN\n    \n    \n      2\n      PAL0910\n      124\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      MALE\n      9.59462\n      -25.42621\n      NaN\n    \n  \n\n\n\n\nNow I have to prepare the qualitative featurs in the data set using pd.get_dummies for Sex and Island and a LabelEncoder for Species. I also removed all irrelevant features:\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nNow if we look at the data we can see that in much better condition to be processed:\n\nX_train.head(3)\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n  \n\n\n\n\n\n\nDisplaying and Discussing Data\nBefore choosing my features, I am going to display and discuss a few of the features in the data set in order to highlight some patterns in the data. First I will use the seaborn library to display the realationsip between flipper length and body mass.\n\nimport seaborn as sns\n\n# Apply the default theme\nsns.set_theme()\n\n# Remove penguin who's sex was not identified\ntrain = train[train[\"Sex\"] != \".\"]\n\n# Create a visualization\nsns.relplot(\n    data=train,\n    x=\"Flipper Length (mm)\", y=\"Body Mass (g)\", col=\"Sex\",\n    hue=\"Species\"\n)\n\n<seaborn.axisgrid.FacetGrid at 0x7fb7956e5a30>\n\n\n\n\n\nFrom this data we can observe a few interesting patterns. The first is that Gentoo penguins are can be easily identified by their body mass and flipper length. The data clearly shows that Gentoo penguins are heavier penguins with larger flippers. Not only are they easily identifiable, the two data sets are linearly seperable (Gento and not Gentoo), given we know the sex of the penguin.\nWe can see also see that Adelie and Chinstrap penguins have very similar body masses and flipper lengths and are therefore not easily identifiable from eachother using those two features.\nThis would mean that in our model, these two factors (body mass and flipper length), although good at identifying Gentoo penguins from non Gentoo penguins, would probably not be good at identifying Adelie from Chinstrap.\nOne other interesting obsrvation we can draw from this graph is that the male penguins tend to be larger then the female penguins.\n\nNow I will take a look at the amount of each species of Penguin on each island.\n\ntrain.groupby([\"Island\", \"Species\"])[[\"Species\"]].count().unstack(fill_value=0).stack()\n\n\n\n\n\n  \n    \n      \n      \n      Species\n    \n    \n      Island\n      Species\n      \n    \n  \n  \n    \n      Biscoe\n      Adelie Penguin (Pygoscelis adeliae)\n      35\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      0\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      100\n    \n    \n      Dream\n      Adelie Penguin (Pygoscelis adeliae)\n      41\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      56\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      0\n    \n    \n      Torgersen\n      Adelie Penguin (Pygoscelis adeliae)\n      42\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      0\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      0\n    \n  \n\n\n\n\nFrom this graph we can see that each species is found on each island in very different amounts. If a penguin is from Biscoe island, it is very unlikely it will be Chinstrap (the data is just a sample so there may be some on the island). Likewise, it would be unlikely to find Gentoo on Dream Island and a Chinstrap or Gentoo on Torgersen island.\nThis also shows that more penguins were surveyed from Biscoe than Dream and many more were surveyed from Dream than Torgersen. This may lead to our clasification algorithm better classifying penguins from Biscoe when compared to those form Dream and Torgersen.\n\n\nChoosing Features\nIn order to choose the three features that could best classify by data, I used sklearn to implement cross validation using linear regression. I first found all possible combinations of one qualitative column and two quantitative columns and then calculated the scores of each combination. Lastly I found the combination with the highest score.\n\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\nall_qual_cols = [\"Clutch Completion_No\", \"Clutch Completion_Yes\", \"Sex_FEMALE\", \"Sex_MALE\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nall_quant_cols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]\n\nscored_cols = []\nLR = LogisticRegression(max_iter = 750)\n\n# Find all posible combinations of one qual. column and two quant. columns\nfor qual in all_qual_cols: \n    qual_cols = [col for col in X_train.columns if qual in col ]\n    for pair in combinations(all_quant_cols, 2):\n        cols = qual_cols + list(pair)\n        # Falculate each combination's scores using linear regression and cross validation\n        scores = cross_val_score(LR, X_train[cols], y_train, cv=5)\n        scored_cols.append([cols, scores.mean()])\n\nbest_score = 0\n\n# Find combination w/ highest score\nfor row in scored_cols:\n    if row[1] > best_score:\n        best_cols = row[0]\n        best_score = row[1]\n    \nprint(best_cols, best_score)\n\n['Island_Dream', 'Culmen Length (mm)', 'Culmen Depth (mm)'] 0.988310708898944\n\n\nAs we can see the Island_Dream, Culmen Length (mm), and Culmen Depth (mm) columns seem to do the best job at classifying the data. The image below by @allison_horst shows what the culmen length and depth refer to.\n\n\n\nCulmen length and depth refers to the length and height of the penguin’s beak\n\n\n\n\nTraining and Ploting Data (Logistic Regression)\nFinally, I will train the logistic regression model on the data. In order to do so, I created a new column called Island_Dream_No which is the opposite of the Island_Dream column. That way I was able desplay all of the data using the plot_regions function defined here.\n\nfrom plot_regions import plot_regions\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Create column that is the opposite of \"Island_Dream\" in order to plot regions\nnot_Island_Dream = X_train[[\"Island_Biscoe\", \"Island_Torgersen\"]].sum(axis=1)\nX_train[\"Island_Dream_No\"] = not_Island_Dream\n\ncols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Island_Dream\", \"Island_Dream_No\"]\n\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\nscore = LR.score(X_train[cols], y_train)\n\nwarnings.filterwarnings(\"default\", category=FutureWarning)\n\nprint(score)\n\nplot_regions(LR, X_train[cols], y_train)\n\n1.0\n\n\n\n\n\nBecause the data is linearly seperable, we are able to reach a score of 1.0. It is also important to note that the data does not seem to be overfit. I think for this reason, linear regression without feature mapping is probably a good choice for a model as it does not “hug” the data. Furthermore, the data seems to be shaped in blobs that can be roughly seperated by linear decision boundries.\n\n\nTest Data\nFinaly, I am going to test to the model on our testing data. I loadd and prepared the data similarly to the training data, ran it thorugh the logistic regression model, then showed the score and plots of this data.\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\nnot_Island_Dream = X_test[[\"Island_Biscoe\", \"Island_Torgersen\"]].sum(axis=1)\nX_test[\"Island_Dream_No\"] = not_Island_Dream\n\nscore = LR.score(X_test[cols], y_test)\nprint(score)\n\nplot_regions(LR, X_test[cols], y_test)\n\n0.9852941176470589\n\n\n\n\n\nWith a score of roughly 98%, this model is very acurate. We can see that the model likely did not overfit to the training data beause it only incorrectly predicted one penguin that was just above the decesion line for it’s correct species."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Weekly Blogs\n\n\n\n\nIn this blog post I will be classifing Palmer Penguins based on a select set of features.\n\n\n\n\n\n\nApr 3, 2023\n\n\nAidan McMillan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nWeekly Blogs\n\n\n\n\nIn this blog post I will be implementing logistic regression.\n\n\n\n\n\n\nMar 9, 2023\n\n\nAidan McMillan\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nWeekly Blogs\n\n\n\n\nIn this blog post I will be implementing the perceptron algorithm that we learned in the first week of CSCI 0451.\n\n\n\n\n\n\nFeb 25, 2023\n\n\nAidan McMillan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is Aidan McMillan’s CSCI 0451 blog"
  }
]