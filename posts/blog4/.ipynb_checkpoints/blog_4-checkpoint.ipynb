{
 "cells": [
  {
   "cell_type": "raw",
   "id": "11bdbd53-984f-472e-881b-3613f0b882a6",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Blog 8: Learning From Timnit Gebru\"\n",
    "author: Aidan McMillan\n",
    "date: '2023-03-09'\n",
    "categories:\n",
    "    - Weekly Blogs\n",
    "image: \"image.jpg\"\n",
    "description: \"Blog post on Timnit Gebru's virtual visit to Middlebury College.\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a4bc7-d3b3-4979-acea-5153d1b82f1b",
   "metadata": {},
   "source": [
    "# Implementation of Stochastic Gradient Descent and Momentum:\n",
    "\n",
    "In order to implement stochastic gradient descent, I created a vector containing numbers 0-n (where n is the number of rows in $\\tilde{\\mathbf{X}}$ using ```np.order(n)``` and then randomly shuffled the numbers in the vector.\n",
    "\n",
    "I then took batches of length ```batch_size```, calculated the gradient descent on each of those batches, and took a step down the gradient using that calculation.\n",
    "\n",
    "For momentum, I stored the last $\\mathbf{w}$ as ```prev_w``` and then calculated the momentum step as ```0.8 * (self.w - prev_w)```. If ```momentum``` is set true by the user, then both the momentum step and gradient step are subtracted from $\\mathbf{w}$ in each iteration. Otherwise, only the gradient is subtracted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml-0451] *",
   "language": "python",
   "name": "conda-env-ml-0451-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
